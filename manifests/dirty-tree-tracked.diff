diff --git a/AGICE-Public/CITATION.cff b/AGICE-Public/CITATION.cff
index 02137c6..73101d3 100644
--- a/AGICE-Public/CITATION.cff
+++ b/AGICE-Public/CITATION.cff
@@ -1,15 +1,11 @@
 cff-version: 1.2.0
-message: "If you use this work, please cite it as below."
-title: "AGICE: Evidence-Native Verifier-Driven Convergence (Public Release)"
+message: "If you use this software, please cite it as below."
+title: "AGICE Demo Replay"
 type: software
+doi: "10.5281/zenodo.18662294"
+url: "https://github.com/TechStrike-ai/AGICE-Public"
+repository-code: "https://github.com/TechStrike-ai/AGICE-Public"
+license: "CC-BY-4.0 AND MIT"
 authors:
-  - family-names: El Mohammady
-    given-names: Ahmed
-repository-code: "https://github.com/TechStrike/AGICE-Public"
-url: "https://techstrike.io/"
-abstract: "AGICE couples a generator (LLM) with verifier packs and evidence-native convergence to produce verified artifacts with audit-grade replayable evidence bundles."
-identifiers:
-  - type: doi
-    value: "10.5281/zenodo.18608886"
-    description: "AGICE Evidence Pack v1 (Zenodo DOI)"
-license: "TBD"
+  - family-names: "El Mohammady"
+    given-names: "Ahmed"
diff --git a/AGICE_Patent_v18.docx b/AGICE_Patent_v18.docx
deleted file mode 100644
index 197f5ac..0000000
Binary files a/AGICE_Patent_v18.docx and /dev/null differ
diff --git a/LCB_BugPack_HARD2_Integration_Plan.md b/LCB_BugPack_HARD2_Integration_Plan.md
deleted file mode 100644
index 2ea2ec7..0000000
--- a/LCB_BugPack_HARD2_Integration_Plan.md
+++ /dev/null
@@ -1,506 +0,0 @@
-# LCB BugPack “HARD2” Integration Plan — Two Ambiguous Geometric-Flip Traps for A0–A3
-
-This document updates the prior CPER-10 plan and replaces it with a **two-operator “HARD2” BugPack** designed specifically to create **ambiguity** where a solver often needs to **try discrete geometric flips** (quarter-turn / basis state) to succeed under a strict fixed budget.
-
-**Important honesty note:** even these are not an absolute guarantee that any strong model will fail. The point is to construct cases that are **information-poor** (from prompt + minimal feedback) and therefore tend to require **explicit enumeration** of 4 discrete states — which your AGICE A3 bounded projection branching is built to do.
-
----
-
-## Goal
-
-- Keep **existing LCB task IDs** (public tasks).
-- Create a deterministic **HARD2 BugPack** that injects one of **two** carefully chosen ambiguity-inducing bugs into the **starting code**.
-- Run your existing **A0–A1–A2–A3** ladder on the same stack.
-- Publish **slice manifest + bugpack manifest + hashes/signatures** so anyone can verify.
-
----
-
-## 1) Non-drift caps (hard requirements)
-
-Put these into config **and enforce in code**. Do not let anyone drift.
-
-### Baseline caps (same for A0–A3)
-
-- `patch_size_limit_lines: 80`
-- `max_verifier_seconds: 180`
-
-### HARD2 demo caps (recommended for “needs enumeration” behavior)
-
-These are the key to making ambiguity bite. Keep these identical across A0–A3.
-
-- `max_rounds_per_task: 6`
-- `max_llm_calls_per_task: 6`  ✅ **HARD CAP** (A3 branching spends from this same pool)
-- `max_total_verifier_calls_per_task: 6` ✅ **HARD CAP**
-
-### Prompt growth control
-
-- `max_prompt_chars: 120000`
-- `max_log_chars_per_verifier: 20000`
-
-### Slice rule (post-cutoff proof)
-
-- Only include tasks with date ≥ `2024-06-01`
-- Log the final selected tasks list into the slice manifest
-
----
-
-## 2) The ambiguity conditions (critical)
-
-To achieve “basis/flip ambiguity” you must ensure the evaluator feedback is **uninformative** about which of the 4 states is correct.
-
-**Do this:**
-1) Verifier returns only **PASS / FAIL** (or strictly limited failure summaries).
-2) Truncate logs deterministically (`max_log_chars_per_verifier`) so hints don’t leak.
-3) Ensure the injected bug does **NOT** break sample I/O in the prompt (if the prompt includes samples).
-   - The code should still “look right” on samples but fail hidden judge.
-4) Prefer tasks where the transform choice is **one of 4 discrete states**:
-   - `{ 0, +π/2, π, -π/2 }` or `{ base, -i, -1, +i }`.
-
-If your verifier currently prints rich diffs or counterexamples, HARD2 becomes much easier for any strong model; keep feedback minimal for this slice.
-
----
-
-## 3) What to add to your repo (minimal + clear)
-
-Add:
-
-```
-bench/
-  lcb_bugpack/
-    build_slice.py
-    hard_ops.py
-    inject_hard2.py
-    make_taskspecs.py
-    README.md
-
-configs/
-  lcb_bugpack_hard2.yaml
-
-scripts/
-  build_lcb_bugpack_hard2.sh
-  run_ablation_lcb_bugpack_hard2.sh
-  sign_bugpack_hard2.sh
-```
-
----
-
-## 4) Config: `configs/lcb_bugpack_hard2.yaml`
-
-```yaml
-# configs/lcb_bugpack_hard2.yaml
-# DO NOT drift: identical caps across A0–A3 for defensible comparison.
-
-patch_size_limit_lines: 80
-max_verifier_seconds: 180
-
-# HARD2 caps (tight on purpose)
-max_rounds_per_task: 6
-max_llm_calls_per_task: 6
-max_total_verifier_calls_per_task: 6
-
-# A3 bounded branching
-N_projections_base: 1
-N_projections_max: 3
-stagnation_window_m: 3
-eps_improvement: 1e-6
-
-temperature_base: 0.4
-temperature_safe_mode: 0.2
-
-max_prompt_chars: 120000
-max_log_chars_per_verifier: 20000
-
-slice_id: post_cutoff_v6_bugpack_hard2
-release_version: release_v6
-start_date: "2024-06-01"
-end_date: null
-
-# Start with 1 seed for iteration speed; expand to 3 later for robustness
-seeds: [20260207]
-```
-
----
-
-## 5) Scripts (coder runs exactly these)
-
-### `scripts/build_lcb_bugpack_hard2.sh`
-
-```bash
-#!/usr/bin/env bash
-set -euo pipefail
-
-CONFIG="configs/lcb_bugpack_hard2.yaml"
-
-# 1) Build HARD2 slice manifest (post-cutoff + filter for the two hard patterns)
-python -m bench.lcb_bugpack.build_slice   --config "${CONFIG}"   --out "bench/slices/post_cutoff_v6_bugpack_hard2.json"   --mode "hard2"
-
-# 2) Inject HARD2 bugs deterministically into starting code per task
-python -m bench.lcb_bugpack.inject_hard2   --config "${CONFIG}"   --slice "bench/slices/post_cutoff_v6_bugpack_hard2.json"   --out_dir "bench/fixtures/lcb_bugpack/hard2_v1"   --start_code_dir "bench/fixtures/lcb_starting_code"   --seed 20260207
-
-# 3) Materialize TaskSpecs for your existing orchestrator
-python -m bench.lcb_bugpack.make_taskspecs   --config "${CONFIG}"   --fixture_dir "bench/fixtures/lcb_bugpack/hard2_v1"   --out "bench/slices/post_cutoff_v6_bugpack_hard2.taskspecs.json"
-```
-
-### `scripts/run_ablation_lcb_bugpack_hard2.sh`
-
-```bash
-#!/usr/bin/env bash
-set -euo pipefail
-
-CONFIG="configs/lcb_bugpack_hard2.yaml"
-SLICE="bench/slices/post_cutoff_v6_bugpack_hard2.taskspecs.json"
-
-python -m bench.run_ablation   --config "${CONFIG}"   --slice_file "${SLICE}"   --ladder "A0,A1,A2,A3"
-```
-
----
-
-## 6) HARD2 operators (only TWO)
-
-### Operator A — “J-cycle shift” (4-state complex basis trap)
-
-**Idea:** If the starting code uses complex quarter-turn logic (`1j`, `-1j`, `-1` multipliers), inject a bug that shifts the basis by one state in the cycle:
-
-`base → -i → -1 → +i → base`
-
-**Why it’s ambiguous:** Without informative feedback, the correct state is effectively one of 4 discrete choices; solving often requires trying them.
-
-#### Selection filter (apply only if true)
-- Starting code contains ≥2 occurrences of `1j`/`-1j`/`-1` in a multiplier context (e.g., `* 1j`, `*= -1j`).
-- If the prompt has samples, the injected bug must **still pass** sample I/O (optional but strongly recommended).
-- After injection it must fail the judge/verifier.
-
-### Operator B — “atan2 quadrant shift” (+π/2 trap)
-
-**Idea:** For tasks using `atan2` and angle logic, inject `+π/2` into the first `atan2` call:
-
-Correct offset ∈ `{0, +π/2, π, -π/2}`
-
-**Why it’s ambiguous:** “Wrong answer” doesn’t reveal which of the 4 quadrant states is needed; enumeration is reliable.
-
-#### Selection filter (apply only if true)
-- Starting code contains `math.atan2(`
-- And contains angle normalization/logic (examples: `% (2*math.pi)`, `while theta < 0`, sorting by angle, quadrant checks).
-- If prompt has samples, bug must still pass samples (recommended).
-- After injection it must fail judge/verifier.
-
----
-
-## 7) Code files (copy/paste)
-
-### `bench/lcb_bugpack/hard_ops.py`
-
-```python
-# bench/lcb_bugpack/hard_ops.py
-import re
-from dataclasses import dataclass
-from typing import Callable, Dict, Tuple
-
-@dataclass(frozen=True)
-class BugOp:
-    bug_id: str
-    description: str
-    apply: Callable[[str], Tuple[str, bool, str]]  # (new_code, changed?, note)
-    predicate: Callable[[str], bool]               # selection filter
-
-def _has_complex_quarter_turns(code: str) -> bool:
-    # heuristic: require multiple occurrences of quarter-turn tokens in multiplier contexts
-    hits = 0
-    for pat in [r"\*\s*1j\b", r"\*\s*-1j\b", r"\*\s*-1\b", r"\*\=\s*1j\b", r"\*\=\s*-1j\b", r"\*\=\s*-1\b"]:
-        hits += len(re.findall(pat, code))
-    return hits >= 2
-
-def _has_atan2_angle_logic(code: str) -> bool:
-    if "math.atan2" not in code:
-        return False
-    angle_markers = [
-        "2*math.pi", "math.pi*2", "% (2*math.pi)", "%(2*math.pi)",
-        "while theta", "while angle", "if theta", "if angle",
-        "sorted(", "sort(", "key=", "quadrant", "normalize"
-    ]
-    return any(m in code for m in angle_markers)
-
-def HARD_J_CYCLE_SHIFT(code: str):
-    """
-    Shift complex quarter-turn basis by one step:
-      1    -> -1j
-      -1j  -> -1
-      -1   ->  1j
-      1j   ->  1
-    Only touches common multiplier forms to avoid wrecking code.
-    """
-    out = code
-    changed = False
-
-    # Apply at most a few local edits to keep the bug realistic and patch small.
-    rules = [
-        (r"\*\s*1j\b", "*(-1j)"),
-        (r"\*\s*-1j\b", "*(-1)"),
-        (r"\*\s*-1\b", "*(1j)"),
-        (r"\*\=\s*1j\b", "*=(-1j)"),
-        (r"\*\=\s*-1j\b", "*=(-1)"),
-        (r"\*\=\s*-1\b", "*=(1j)"),
-    ]
-
-    for pat, rep in rules:
-        out2, n = re.subn(pat, rep, out, count=1)
-        if n > 0:
-            out = out2
-            changed = True
-
-    return out, changed, "shifted complex basis by one J-cycle step"
-
-def HARD_ATAN2_QUADRANT_SHIFT(code: str):
-    """
-    Replace first math.atan2(y,x) with math.atan2(y,x) + (pi/2).
-    Fix requires choosing correct offset among {0, +pi/2, pi, -pi/2}.
-    """
-    pat = r"math\.atan2\(\s*([^,]+)\s*,\s*([^)]+)\s*\)"
-    m = re.search(pat, code)
-    if not m:
-        return code, False, "no atan2 match"
-
-    orig = m.group(0)
-    shifted = f"({orig} + (math.pi/2))"
-    out = code.replace(orig, shifted, 1)
-    return out, True, "shifted atan2 by +pi/2 (quadrant trap)"
-
-HARD_OPS: Dict[str, BugOp] = {
-    "HARD_J_CYCLE_SHIFT": BugOp(
-        bug_id="HARD_J_CYCLE_SHIFT",
-        description="Shift complex quarter-turn basis by one J-cycle step (4-state ambiguity)",
-        apply=HARD_J_CYCLE_SHIFT,
-        predicate=_has_complex_quarter_turns,
-    ),
-    "HARD_ATAN2_QUADRANT_SHIFT": BugOp(
-        bug_id="HARD_ATAN2_QUADRANT_SHIFT",
-        description="Shift atan2 output by +pi/2 (4-state quadrant ambiguity)",
-        apply=HARD_ATAN2_QUADRANT_SHIFT,
-        predicate=_has_atan2_angle_logic,
-    ),
-}
-```
-
----
-
-### `bench/lcb_bugpack/inject_hard2.py`
-
-Assumptions:
-- You already store starting code here: `bench/fixtures/lcb_starting_code/<question_id>.py`
-- Your verifier can evaluate a single LCB task ID on a given solution file (same as your current stack)
-
-This injector:
-- selects a HARD2 op deterministically,
-- applies it only if the predicate matches,
-- writes fixture + manifest.
-
-```python
-# bench/lcb_bugpack/inject_hard2.py
-import argparse, json, hashlib
-from pathlib import Path
-
-from bench.lcb_bugpack.hard_ops import HARD_OPS
-
-def sha256_bytes(b: bytes) -> str:
-    return hashlib.sha256(b).hexdigest()
-
-def read_text(p: Path) -> str:
-    return p.read_text(encoding="utf-8")
-
-def write_text(p: Path, s: str):
-    p.parent.mkdir(parents=True, exist_ok=True)
-    p.write_text(s, encoding="utf-8")
-
-def main():
-    ap = argparse.ArgumentParser()
-    ap.add_argument("--config", required=True)
-    ap.add_argument("--slice", required=True)
-    ap.add_argument("--out_dir", required=True)
-    ap.add_argument("--start_code_dir", required=True)
-    ap.add_argument("--seed", type=int, required=True)
-    args = ap.parse_args()
-
-    import yaml
-    cfg = yaml.safe_load(open(args.config, "r", encoding="utf-8"))
-    sl = json.load(open(args.slice, "r", encoding="utf-8"))
-    tasks = sl["tasks"]
-
-    bug_ids = list(HARD_OPS.keys())
-
-    out_dir = Path(args.out_dir)
-    out_dir.mkdir(parents=True, exist_ok=True)
-
-    manifest = {
-        "slice_id": cfg["slice_id"],
-        "release_version": cfg["release_version"],
-        "start_date": cfg["start_date"],
-        "seed": args.seed,
-        "bug_set": bug_ids,
-        "items": []
-    }
-
-    for t in tasks:
-        qid = t["question_id"]
-        start_path = Path(args.start_code_dir) / f"{qid}.py"
-        if not start_path.exists():
-            manifest["items"].append({"question_id": qid, "status": "SKIP_NO_START_CODE"})
-            continue
-
-        start_code = read_text(start_path)
-
-        # Deterministic op choice by hash(qid) (publicly reproducible)
-        h = int(hashlib.sha256(qid.encode()).hexdigest(), 16)
-        bug_id = bug_ids[h % len(bug_ids)]
-        op = HARD_OPS[bug_id]
-
-        # Apply only if selection filter matches
-        if not op.predicate(start_code):
-            manifest["items"].append({"question_id": qid, "status": "SKIP_PREDICATE_FALSE", "bug_id": bug_id})
-            continue
-
-        buggy, changed, note = op.apply(start_code)
-        if not changed:
-            manifest["items"].append({"question_id": qid, "status": "SKIP_NO_CHANGE", "bug_id": bug_id, "note": note})
-            continue
-
-        task_dir = out_dir / qid
-        task_dir.mkdir(parents=True, exist_ok=True)
-
-        write_text(task_dir / "solution.py", buggy)
-        write_text(task_dir / "meta.json", json.dumps({
-            "question_id": qid,
-            "release_date": t["release_date"],
-            "bug_id": bug_id,
-            "bug_note": note,
-            "start_code_sha256": sha256_bytes(start_code.encode()),
-            "buggy_code_sha256": sha256_bytes(buggy.encode()),
-        }, indent=2))
-
-        manifest["items"].append({
-            "question_id": qid,
-            "status": "OK",
-            "bug_id": bug_id,
-            "start_code_sha256": sha256_bytes(start_code.encode()),
-            "buggy_code_sha256": sha256_bytes(buggy.encode()),
-        })
-
-    write_text(out_dir / "bugpack_manifest.json", json.dumps(manifest, indent=2))
-    print(f"Wrote HARD2 bugpack to {out_dir}")
-
-if __name__ == "__main__":
-    main()
-```
-
----
-
-### `bench/lcb_bugpack/make_taskspecs.py`
-
-Converts fixtures into TaskSpecs for your existing `bench.run_ablation`.
-
-**IMPORTANT:** The verifier command array is a placeholder hook. Your repo already has a verifier adapter—swap only the command array if needed.
-
-```python
-# bench/lcb_bugpack/make_taskspecs.py
-import argparse, json, os
-from pathlib import Path
-
-def main():
-    ap = argparse.ArgumentParser()
-    ap.add_argument("--config", required=True)
-    ap.add_argument("--fixture_dir", required=True)
-    ap.add_argument("--out", required=True)
-    args = ap.parse_args()
-
-    fixture_dir = Path(args.fixture_dir)
-    tasks = []
-
-    for task_dir in sorted(p for p in fixture_dir.iterdir() if p.is_dir()):
-        meta_p = task_dir / "meta.json"
-        sol_p = task_dir / "solution.py"
-        if not (meta_p.exists() and sol_p.exists()):
-            continue
-
-        meta = json.loads(meta_p.read_text(encoding="utf-8"))
-        qid = meta["question_id"]
-
-        tasks.append({
-            "task_id": f"lcb_bugpack_hard2::{qid}",
-            "question_id": qid,
-            "repo_dir": str(task_dir),
-            "entry_file": "solution.py",
-            "invariants": [
-                "Only modify solution.py",
-                "Do not modify meta.json",
-            ],
-            "verifier_pack": {
-                "V_tests": {
-                    "hard_constraint": True,
-                    "cmd": [
-                        "python", "-m", "bench.livecodebench", "verify_one",
-                        "--question_id", qid,
-                        "--solution_path", "solution.py"
-                    ],
-                }
-            }
-        })
-
-    os.makedirs(os.path.dirname(args.out), exist_ok=True)
-    Path(args.out).write_text(json.dumps({"tasks": tasks}, indent=2), encoding="utf-8")
-    print(f"Wrote {len(tasks)} TaskSpecs to {args.out}")
-
-if __name__ == "__main__":
-    main()
-```
-
----
-
-## 8) Hashing + signing for public verification
-
-### `scripts/sign_bugpack_hard2.sh`
-
-```bash
-#!/usr/bin/env bash
-set -euo pipefail
-
-BUGPACK_DIR="bench/fixtures/lcb_bugpack/hard2_v1"
-
-tar --sort=name --mtime='UTC 2026-02-07' --owner=0 --group=0 --numeric-owner   -cf /tmp/lcb_bugpack_hard2_v1.tar -C "${BUGPACK_DIR}" .
-
-sha256sum /tmp/lcb_bugpack_hard2_v1.tar | tee "${BUGPACK_DIR}/BUGPACK_TARBALL_SHA256.txt"
-
-# Optional signatures (choose one):
-# minisign -Sm /tmp/lcb_bugpack_hard2_v1.tar
-# cosign sign-blob /tmp/lcb_bugpack_hard2_v1.tar > "${BUGPACK_DIR}/BUGPACK_TARBALL.sig"
-```
-
-Publish:
-- Slice manifest: `bench/slices/post_cutoff_v6_bugpack_hard2.json`
-- Bugpack manifest: `bench/fixtures/lcb_bugpack/hard2_v1/bugpack_manifest.json`
-- Tarball hash (+ signature)
-- Exact config YAML
-- Your run logs + evidence bundle hashes
-
----
-
-## 9) Integration note (only likely adjustment)
-
-This plan assumes starting code is here:
-
-- `bench/fixtures/lcb_starting_code/<question_id>.py`
-
-If your repo uses a different path or naming:
-- update `--start_code_dir` and `start_path` mapping in `inject_hard2.py`
-- leave everything else unchanged
-
----
-
-## Optional (recommended): Sample I/O “ambiguity gate”
-
-If you want HARD2 to be maximally convincing, implement a gate that ensures:
-- **starting code** matches prompt samples
-- **buggy code** still matches prompt samples
-- but the judge fails on hidden tests
-
-This prevents the injected bug from being “obvious” from the prompt alone and forces reliance on enumeration.
-
-Implementation can be added later; the rest of this plan works without it.
diff --git a/README.md b/README.md
index 68b88e3..c8b7882 100644
--- a/README.md
+++ b/README.md
@@ -1,88 +1,196 @@
-# AGICE — Deployment Plan v2
+# AGICEv3 — Evidence-Native Convergence Engine
 
-**AGICE Reasoning Model**: verification-driven convergence for code generation.
-Implements the full closed-loop orchestrator from AGICE Patent v18 / Deployment Plan v2.
+AGICE is a verifier-driven, evidence-native convergence loop for code generation/reasoning tasks.
+
+It supports ablation modes `A0..A3`, projection branching (`−i` family), and replayable evidence bundles for audit-grade evaluation.
+
+Drop 2 is now integrated with:
+- hybrid JSON feedback to the generator,
+- multi-sample candidates per projection,
+- optional static-analysis and property-based verifier channels,
+- artifact embeddings + retrieval memory (with graceful fallbacks).
+
+## Start Here
+
+The primary onboarding document is:
+
+- `handoff.md`
+
+Then read:
+
+- `architecture.md`
+- `gap-analysis.md`
+- `Reports-archive/testing-drop1.md`
+- `Reports-archive/testing-drop1-results.md`
+- `zenodo-bundle-drop2.md`
+
+`AGICE_v19.docx` is the patent/reference-architecture source.
 
 ## Quickstart
 
 ```bash
-# 1) Install
-python -m venv .venv && source .venv/bin/activate
+# 1) Create virtual environment
+python -m venv .venv
+source .venv/bin/activate
+
+# 2) Install dependencies
 pip install -r requirements.txt
 
-# 2) Set OpenAI API key
+# 3) Set OpenAI API key
 export OPENAI_API_KEY="sk-..."
 
-# 3) Run a single task
-python -m agice run_one --config configs/default.yaml --task_id lcb_v6_001
+# 4) Run one task (default ablation A3)
+python -m agice run_one \
+  --config configs/default.yaml \
+  --task_family livecodebench \
+  --slice sweet_spot_drop2_web_v2 \
+  --task_id ss_010
+
+# 5) Run ablation ladder on a slice
+python -m bench.run_ablation \
+  --config configs/ablation_4o_mini.yaml \
+  --task_family livecodebench \
+  --slice sweet_spot_drop2_web_v2 \
+  --seed 42
+
+# 5b) Drop 2 full-stack profile
+python -m bench.run_ablation \
+  --config configs/drop2_fullstack.yaml \
+  --task_family livecodebench \
+  --slice sweet_spot_drop2_web_v2 \
+  --seed 42
+
+# 6) Validate evidence before using results
+python -m evidence.validate_bundles --root runs
+python -m evidence.audit_metrics --runs_root runs
+python -m evidence.replay --bundle_dir runs/<run_id>/A3/bundles
+
+# 7) Build paper-ready executive CSV (public names, no internal case IDs)
+python -m evidence.executive_csv \
+  --long-csv Reports-archive/testing-drop1-plus-o3-demo-long.csv \
+  --out Reports-archive/testing-drop1-executive-summary.csv
+
+# 8) Build public replay package (executive CSV + manifest + validation report)
+python -m evidence.demo_replay \
+  --long-csv Reports-archive/testing-drop1-plus-o3-demo-long.csv \
+  --out-dir artifacts/demo-replay \
+  --run-replay
+```
 
-# 4) Run full ablation ladder (A0-A3) on the benchmark slice
-python -m bench.run_ablation --config configs/default.yaml --slice post_cutoff_v6_small
+## Core Modes (Ablation Ladder)
 
-# 5) Replay/verify evidence bundles
-python -m evidence.replay --bundle_dir runs/<run_id>/A3/bundles --max_tasks 10
-```
+| Mode | Description |
+|------|-------------|
+| **A0** | One-shot LLM baseline (no retries) |
+| **A1** | Retry baseline with unstructured feedback |
+| **A2** | Structured diagnostics + arbitration (no projection branching) |
+| **A3** | A2 + bounded projection branching (`−i` family / MA scaffolding) |
+
+## Repository Map
+
+### Runtime
+
+- `agice/orchestrator.py` — closed-loop execution and mode behavior
+- `agice/projections.py` — projection logic (`base`, `minus_i`, etc.)
+- `agice/arbiter.py` — hard/soft selection rules
+- `agice/critic.py` — diagnostics normalization
+- `agice/hint_packet.py` — generator hint assembly
+- `agice/feedback.py` — deterministic structured feedback payloads
+- `agice/contracts.py` — schema contracts
+- `agice/embeddings.py` — artifact embeddings + vector memory index
+
+### Drop 1 support modules
+
+- `agice/delta_metrics.py` — bounded-change contract metrics
+- `agice/distance.py` — distance/medoid tie-break utilities
+- `agice/provenance.py` — config/environment identity hashing
+
+### Benchmark + verification + evidence
+
+- `bench/run_ablation.py` — official `A0..A3` runner
+- `bench/task_loader.py` — multi-family task loader (`livecodebench`, `swebench`)
+- `bench/livecodebench.py` — slice/task loader
+- `verifier/runner.py` — test execution and diagnostics
+- `evidence/bundle.py` — evidence bundle writer
+- `evidence/replay.py` — replay with verifier re-run
+- `evidence/validate_bundles.py` — bundle integrity/schema checks
+- `evidence/audit_metrics.py` — summary consistency checks
+- `evidence/governance_report.py` — rolling-window governance metrics
+
+## Key Configs
+
+- `configs/default.yaml` — default single-task run config
+- `configs/ablation_4o_mini.yaml` — recommended for convergence/projection testing
+- `configs/drop2_fullstack.yaml` — full Drop 2 profile (hybrid JSON + P0/P1 channels)
+- `configs/ablation_gpt35.yaml`
+- `configs/ablation_o3.yaml`
 
-## Architecture (Plan v2)
+## Common Slices
 
+From `cache/livecodebench/slices/`:
+
+- `sweet_spot_drop2_web_v2` — Drop 2 primary sweet-spot slice (web-researched + evidence-informed)
+- `sweet_spot_v2` — current high-signal slice for A3 projection behavior
+- `official_9task` — control/drift tracking slice
+- `a1_filter_candidates` — useful for A2/A3 vs A1 pressure tests
+- `circ_only` — single-task calibration slice (`ss_005`)
+
+## Reports Archive
+
+All generated CSV analysis reports are archived under:
+
+- `Reports-archive/`
+
+Primary files for publication/replay:
+
+- `Reports-archive/testing-drop1-plus-o3-demo-long.csv` — merged long-form run index (includes o3 `lcb_v6_003` demo case)
+- `Reports-archive/testing-drop1-executive-summary.csv` — publication-ready executive table (`AGICE Demo Replay`)
+
+## Evidence Bundle Structure
+
+Each task run writes:
+
+`runs/<run_id>/<ablation>/bundles/<task_id>/`
+
+Typical files:
+
+- `events.jsonl` — append-only hash-chained event log
+- `bundle_manifest.json` — per-task summary + provenance identifiers
+- `integrity.json` — recorded file hashes
+- `replay.json` / `replay_report.json` — replay instructions/results
+- `artifacts/selected_round_*.py` — selected artifacts per round
+
+## Zenodo Bundle Workflow
+
+Build a signed, offline-verifiable Zenodo artifact from a replay CSV:
+
+```bash
+python -m evidence.zenodo_bundle build \
+  --long-csv Reports-archive/testing-drop1-plus-o3-demo-long.csv \
+  --executive-csv Reports-archive/testing-drop1-executive-summary.csv \
+  --overall-report Reports-archive/testing-drop1-results.md \
+  --trajectory-md Reports-archive/testing-drop1-a3-trajectory-pack.md \
+  --trajectory-csv Reports-archive/testing-drop1-a3-trajectory-pack.csv \
+  --doi 10.5281/zenodo.18662294 \
+  --out-dir artifacts/zenodo/agice-demo-replay-v1 \
+  --generate-key-if-missing \
+  --clean
 ```
-Task + Verifier Pack
-        │
-        ▼
-┌─────────────────────────────────────────────┐
-│  Orchestrator (closed-loop, max R rounds)   │
-│  ┌──────────┐  ┌──────────┐  ┌───────────┐ │
-│  │HintPacket│→ │Generator │→ │RoundOutput│ │
-│  │ builder  │  │ (LLM)   │  │ (code)    │ │
-│  └──────────┘  └──────────┘  └───────────┘ │
-│       ▲                           │         │
-│       │        ┌──────────┐       ▼         │
-│  ┌────┴────┐   │ Arbiter  │  ┌──────────┐  │
-│  │  Critic │ ← │ (hard +  │← │ Verifier │  │
-│  │(diag)   │   │  soft)   │  │ Pack     │  │
-│  └─────────┘   └──────────┘  └──────────┘  │
-│       │                                     │
-│       ▼  Convergence + Projection (J/MA-i)  │
-│       │  Governance (Wilson LB gate)        │
-│       ▼                                     │
-│  Evidence Bundle (hash-chained, replayable) │
-└─────────────────────────────────────────────┘
+
+Verify the produced bundle offline:
+
+```bash
+python -m evidence.zenodo_bundle verify \
+  --bundle-dir artifacts/zenodo/agice-demo-replay-v1
 ```
 
-## Ablation Ladder
+## Current Status (Drop 2 + Publication)
 
-| Mode | Description |
-|------|-------------|
-| **A0** | LLM-only one-shot (no retries) |
-| **A1** | LLM naive retry (unstructured feedback) |
-| **A2** | AGICE-lite (structured diagnostics + selection, no branching) |
-| **A3** | Full AGICE (A2 + bounded projection branching + multi-oracle) |
-
-## Key Modules
-
-- **`agice/orchestrator.py`** — Main loop (Plan v2 §7), GeneratorClient (OpenAI)
-- **`agice/contracts.py`** — JSON Schema validators (RoundOutput, VerifierOutput, etc.)
-- **`agice/critic.py`** — Deterministic diagnostics normalization
-- **`agice/hint_packet.py`** — Structured feedback builder
-- **`agice/arbiter.py`** — Hard/soft constraint enforcement, composite loss
-- **`agice/projections.py`** — Bounded 4-cycle J operator
-- **`agice/convergence.py`** — Stagnation detection
-- **`verifier/runner.py`** — Subprocess-based code execution against test cases
-- **`evidence/bundle.py`** — Hash-chained events.jsonl + manifest + integrity
-- **`evidence/replay.py`** — Deterministic replay with hash chain verification
-- **`bench/run_ablation.py`** — A0-A3 runner with metrics
-- **`bench/livecodebench.py`** — Task loader, LiveCodeBench output format
-
-## Evidence Bundles
-
-Each task produces `runs/<run_id>/<ablation>/bundles/<task_id>/`:
-- `events.jsonl` — hash-chained append-only event log
-- `bundle_manifest.json` — generator_id, seed, environment, per-round summaries
-- `integrity.json` — content hashes, optional signing hook
-- `replay.json` — deterministic replay instructions
-- `artifacts/` — per-round patches and logs
-
-## Benchmark Slice
-
-`cache/livecodebench/slices/post_cutoff_v6_small.json` contains 10 coding problems
-with public + private test cases. `task_id` = LiveCodeBench `question_id`.
+See `Reports-archive/testing-drop1-results.md` for full details.
+
+At a high level:
+
+- Evidence validation/audit/replay tooling is integrated and in active use.
+- Projection-rotation advantage has been observed and documented with evidence-gated runs.
+- Publication artifacts are generated via the replay/Zenodo pipeline and validated offline.
+- Ongoing work focuses on expanding seed/slice coverage and tightening confidence bounds.
diff --git a/agice/cli.py b/agice/cli.py
index 37ccdeb..16c4744 100644
--- a/agice/cli.py
+++ b/agice/cli.py
@@ -10,13 +10,15 @@ import argparse
 import json
 import logging
 import os
+import time
 from typing import Any, Dict
 
 import yaml
 
 from .orchestrator import OrchestratorConfig, GeneratorClient, run_task
+from .provenance import sha256_file
 from verifier.runner import VerifierRunner, VerifierRunnerConfig
-from bench.livecodebench import load_slice
+from bench.task_loader import load_tasks
 
 logging.basicConfig(
     level=logging.INFO,
@@ -41,25 +43,42 @@ def main(argv=None) -> int:
     p1.add_argument("--config", required=True)
     p1.add_argument("--task_id", required=True)
     p1.add_argument("--slice", default="post_cutoff_v6_small")
+    p1.add_argument("--task_family", default="livecodebench", choices=["livecodebench", "swebench"])
     p1.add_argument("--run_dir", default="runs/local")
     p1.add_argument("--ablation", default="A3", choices=["A0", "A1", "A2", "A3"])
 
     args = p.parse_args(argv)
     cfg = load_config(args.config)
+    os.environ.setdefault("AGICE_RUN_ID", time.strftime("%Y%m%d_%H%M%S"))
 
     vr = VerifierRunner(VerifierRunnerConfig(
         max_seconds=cfg.max_verifier_seconds,
         enable_lint_oracle=cfg.enable_lint_oracle,
+        enable_ruff_oracle=getattr(cfg, "enable_ruff_oracle", False),
+        enable_pyright_oracle=getattr(cfg, "enable_pyright_oracle", False),
+        enable_mypy_oracle=getattr(cfg, "enable_mypy_oracle", False),
+        enable_hypothesis_oracle=getattr(cfg, "enable_hypothesis_oracle", False),
+        static_oracle_timeout_seconds=getattr(cfg, "static_oracle_timeout_seconds", 20),
+        strict_external_oracles=getattr(cfg, "strict_external_oracles", False),
+        hypothesis_examples=getattr(cfg, "hypothesis_examples", 12),
+        ruff_bin=getattr(cfg, "ruff_bin", "ruff"),
+        pyright_bin=getattr(cfg, "pyright_bin", "pyright"),
+        mypy_bin=getattr(cfg, "mypy_bin", "mypy"),
     ))
     gen = GeneratorClient(model=cfg.model, reasoning_effort=cfg.reasoning_effort)
 
-    tasks = load_slice(args.slice)
+    tasks = load_tasks(args.task_family, args.slice)
     task = next((t for t in tasks if t.task_id == args.task_id), None)
     if task is None:
         print(f"Task {args.task_id} not found in slice {args.slice}")
         return 1
 
-    test_cases = task.get_test_cases(include_private=True)
+    include_private = bool(args.task_family == "livecodebench")
+    test_cases = task.get_test_cases(include_private=include_private)
+    if args.task_family == "livecodebench":
+        slice_path = os.path.join("cache", "livecodebench", "slices", f"{args.slice}.json")
+    else:
+        slice_path = os.path.join("cache", "swebench", "slices", f"{args.slice}.json")
     summary = run_task(
         task_id=task.task_id,
         prompt=task.prompt,
@@ -69,6 +88,22 @@ def main(argv=None) -> int:
         generator=gen,
         verifier_runner=vr,
         ablation_mode=args.ablation,
+        provenance={
+            "run_id": os.environ.get("AGICE_RUN_ID"),
+            "slice_id": args.slice,
+            "task_family": args.task_family,
+            "slice_sha256": sha256_file(slice_path),
+            "evaluated_task_ids": [task.task_id],
+            "seed_base": 42,
+            "config_path": args.config,
+            "config_sha256": sha256_file(args.config),
+            "model": cfg.model,
+            "reasoning_effort": cfg.reasoning_effort,
+            "task_manifest": {
+                "repo_url": task.repo_url,
+                "commit": task.commit,
+            },
+        },
     )
     print(json.dumps({k: v for k, v in summary.items() if k != "manifest"}, indent=2, default=str))
     return 0
diff --git a/agice/contracts.py b/agice/contracts.py
index e11957d..5138f6e 100644
--- a/agice/contracts.py
+++ b/agice/contracts.py
@@ -117,6 +117,9 @@ EVIDENCE_MANIFEST_SCHEMA: Dict[str, Any] = {
         "run_id": {"type": "string"},
         "generator_id": {"type": "string"},
         "seed": {"type": "integer"},
+        "config_id": {"type": "string"},
+        "environment_hash": {"type": "string"},
+        "provenance": {"type": "object", "additionalProperties": True},
         "environment": {"type": "object", "additionalProperties": True},
         "rounds": {"type": "array", "items": {"type": "object", "additionalProperties": True}},
         "final_decision": {"type": "object", "additionalProperties": True},
diff --git a/agice/critic.py b/agice/critic.py
index edf4d27..53d2fbf 100644
--- a/agice/critic.py
+++ b/agice/critic.py
@@ -36,6 +36,10 @@ def normalize_diagnostics(verifier_outputs: List[Dict[str, Any]]) -> Dict[str, A
             normalized = _normalize_test_results(raw_diag)
         elif vid == "V_lint":
             normalized = _normalize_lint(raw_diag)
+        elif vid in ("V_ruff", "V_pyright", "V_mypy"):
+            normalized = _normalize_static_tool(vid, raw_diag)
+        elif vid == "V_hypothesis":
+            normalized = _normalize_hypothesis(raw_diag)
         else:
             normalized = {"kind": vid, "raw": raw_diag}
 
@@ -88,12 +92,57 @@ def _normalize_lint(raw: Dict[str, Any]) -> Dict[str, Any]:
     }
 
 
+def _normalize_static_tool(kind: str, raw: Dict[str, Any]) -> Dict[str, Any]:
+    """Normalize static analysis diagnostics (ruff/pyright/mypy)."""
+    errors = raw.get("errors", [])
+    if not isinstance(errors, list):
+        errors = []
+    return {
+        "kind": kind,
+        "unavailable": bool(raw.get("unavailable", False)),
+        "error_count": int(raw.get("error_count", len(errors)) or 0),
+        "warning_count": int(raw.get("warning_count", 0) or 0),
+        "errors": errors[:10],
+    }
+
+
+def _normalize_hypothesis(raw: Dict[str, Any]) -> Dict[str, Any]:
+    failures = raw.get("failures", [])
+    if not isinstance(failures, list):
+        failures = []
+    return {
+        "kind": "hypothesis",
+        "unavailable": bool(raw.get("unavailable", False)),
+        "example_count": int(raw.get("example_count", 0) or 0),
+        "error_count": int(raw.get("error_count", len(failures)) or 0),
+        "failures": failures[:8],
+    }
+
+
 def diagnostics_to_feedback_text(diag: Dict[str, Any]) -> str:
     """
     Convert structured diagnostics into concise text for the LLM hint packet.
     Deterministic: same diagnostics always produce the same text.
     """
     parts: List[str] = []
+    v_contract = diag.get("verifiers", {}).get("V_contract")
+    if v_contract and not v_contract.get("overall_passed", True):
+        raw = v_contract.get("raw", {})
+        bounds = raw.get("bounds", {}) if isinstance(raw, dict) else {}
+        metrics = raw.get("metrics", {}) if isinstance(raw, dict) else {}
+        max_changed = bounds.get("max_changed_lines")
+        changed_total = metrics.get("changed_lines_total")
+        added = metrics.get("added_lines")
+        removed = metrics.get("removed_lines")
+        if max_changed is not None and changed_total is not None:
+            msg = (
+                f"Contract violation: changed {int(changed_total)} lines "
+                f"(+{int(added or 0)}, -{int(removed or 0)}), max allowed is {int(max_changed)}. "
+                "Make a smaller, localized fix."
+            )
+        else:
+            msg = "Contract violation: output exceeded the bounded incremental change limit. Make a smaller fix."
+        parts.append(msg)
     v_tests = diag.get("verifiers", {}).get("V_tests")
     if v_tests:
         parts.append(f"Tests: {v_tests['passed']}/{v_tests['total']} passed.")
@@ -113,4 +162,26 @@ def diagnostics_to_feedback_text(diag: Dict[str, Any]) -> str:
         for e in v_lint.get("errors", [])[:3]:
             parts.append(f"  Line {e.get('line','?')}: {e.get('msg','')}")
 
+    for vid in ("V_ruff", "V_pyright", "V_mypy"):
+        v = diag.get("verifiers", {}).get(vid)
+        if not v:
+            continue
+        if v.get("unavailable", False):
+            continue
+        if not v.get("overall_passed", True):
+            parts.append(
+                f"{vid}: errors={v.get('error_count', 0)} warnings={v.get('warning_count', 0)}."
+            )
+            for e in v.get("errors", [])[:2]:
+                msg = e.get("message", e.get("msg", ""))
+                line = e.get("line", "?")
+                parts.append(f"  {vid} line {line}: {msg}")
+
+    v_h = diag.get("verifiers", {}).get("V_hypothesis")
+    if v_h:
+        if not v_h.get("unavailable", False) and not v_h.get("overall_passed", True):
+            parts.append(
+                f"V_hypothesis: failed {v_h.get('error_count', 0)} / {v_h.get('example_count', 0)} perturbation checks."
+            )
+
     return "\n".join(parts) if parts else "No diagnostics available."
diff --git a/agice/hint_packet.py b/agice/hint_packet.py
index 8832b37..dfd874d 100644
--- a/agice/hint_packet.py
+++ b/agice/hint_packet.py
@@ -15,6 +15,7 @@ from typing import Any, Dict, List, Optional
 
 from .contracts import validate_hint_packet
 from .critic import diagnostics_to_feedback_text
+from .feedback import build_feedback_payload
 
 
 @dataclass(frozen=True)
@@ -36,11 +37,16 @@ def build_hint_packet(
     projection_id: str,
     prompt: str,
     *,
+    contract_bounds: Optional[Dict[str, Any]] = None,
+    contract_feedback_text: Optional[str] = None,
     previous_code: Optional[str] = None,
     diagnostics: Optional[Dict[str, Any]] = None,
     score_prev: Optional[float] = None,
     ablation_mode: str = "A3",
     stagnation_rounds: int = 0,
+    rotation_analysis: Optional[Dict[str, Any]] = None,
+    retrieval_memory: Optional[List[Dict[str, Any]]] = None,
+    feedback_max_failing_tests: int = 2,
 ) -> HintPacket:
     """
     Construct a HintPacket with all context the generator needs.
@@ -50,15 +56,22 @@ def build_hint_packet(
         round_id: Current round number (0-indexed).
         projection_id: Projection state (base, minus_i, neg_base, neg_minus_i).
         prompt: The task problem statement.
+        contract_bounds: Optional contract bounds to enforce (e.g. max changed lines).
+        contract_feedback_text: Optional message about a recent contract violation (for the generator).
         previous_code: Code from the previous round (None for round 0).
         diagnostics: Normalized diagnostics from the Critic (None for round 0).
         score_prev: Composite loss from the previous round.
         ablation_mode: A0/A1/A2/A3 — controls what feedback the LLM receives.
+        rotation_analysis: Structured JSON hint from GMDT geometric projection (A3 only).
     """
     inputs: Dict[str, Any] = {
         "problem_statement": prompt,
         "ablation_mode": ablation_mode,
     }
+    if contract_bounds is not None:
+        inputs["contract_bounds"] = contract_bounds
+    if contract_feedback_text:
+        inputs["contract_feedback_text"] = contract_feedback_text
 
     if round_id == 0:
         inputs["instruction"] = (
@@ -66,6 +79,15 @@ def build_hint_packet(
             "The program should solve the problem described above."
         )
     else:
+        v_tests = (
+            diagnostics.get("verifiers", {}).get("V_tests", {})
+            if isinstance(diagnostics, dict)
+            else {}
+        )
+        failed = int(v_tests.get("failed", 0) or 0) if isinstance(v_tests, dict) else 0
+        total = int(v_tests.get("total", 0) or 0) if isinstance(v_tests, dict) else 0
+        failed_ratio = (failed / total) if total > 0 else 0.0
+
         if stagnation_rounds >= 2 and ablation_mode in ("A2", "A3"):
             inputs["instruction"] = (
                 f"Your approach has stagnated for {stagnation_rounds} consecutive rounds with no improvement. "
@@ -73,6 +95,12 @@ def build_hint_packet(
                 "You MUST try a fundamentally DIFFERENT algorithm or data structure. "
                 "Rethink the problem from scratch. Return a complete, corrected Python program."
             )
+        elif ablation_mode in ("A2", "A3") and failed_ratio >= 0.40:
+            inputs["instruction"] = (
+                "Your previous algorithm fails on many tests. "
+                "Do not patch locally. Re-derive the core logic and implement a different algorithmic strategy "
+                "that satisfies the failing examples. Return a complete, corrected Python program."
+            )
         elif round_id == 1 and previous_code is not None:
             inputs["instruction"] = (
                 "The code below has a bug — it fails some hidden test cases. "
@@ -88,8 +116,20 @@ def build_hint_packet(
             inputs["previous_code"] = previous_code
 
         if diagnostics is not None and ablation_mode in ("A2", "A3"):
-            inputs["diagnostics_text"] = diagnostics_to_feedback_text(diagnostics)
+            feedback_text = diagnostics_to_feedback_text(diagnostics)
+            lines = [line for line in feedback_text.splitlines() if line.strip()]
+            if lines:
+                compact_lines = [lines[0]]
+                compact_lines.extend([line for line in lines[1:] if "FAIL" in line][:2])
+                inputs["diagnostics_text"] = "\n".join(compact_lines)
+            else:
+                inputs["diagnostics_text"] = feedback_text
             inputs["diagnostics_structured"] = diagnostics
+            inputs["feedback_json"] = build_feedback_payload(
+                diagnostics,
+                projection_analysis=rotation_analysis,
+                max_failing_tests=max(1, int(feedback_max_failing_tests)),
+            )
         elif diagnostics is not None and ablation_mode == "A1":
             # Naive retry: only tell the LLM it failed, not WHY or WHICH tests
             v_tests = diagnostics.get("verifiers", {}).get("V_tests", {})
@@ -103,9 +143,13 @@ def build_hint_packet(
 
         if score_prev is not None:
             inputs["previous_loss"] = score_prev
+        if retrieval_memory:
+            inputs["retrieval_memory"] = retrieval_memory[:3]
 
     if ablation_mode == "A3":
         inputs["projection_id"] = projection_id
+        if rotation_analysis is not None:
+            inputs["projection_analysis"] = rotation_analysis
 
     return HintPacket(
         task_id=task_id,
diff --git a/agice/orchestrator.py b/agice/orchestrator.py
index 516240a..88b7e3d 100644
--- a/agice/orchestrator.py
+++ b/agice/orchestrator.py
@@ -32,7 +32,7 @@ load_dotenv()
 
 from .contracts import validate_round_output
 from .hint_packet import HintPacket, build_hint_packet
-from .projections import enumerate_projections, projection_cycle
+from .projections import enumerate_projections, projection_cycle, vembed, build_rotation_analysis
 from .convergence import ConvergenceState, update_and_check_stagnation
 from .arbiter import ArbiterConfig, ArbitrationDecision, arbitrate
 from .critic import normalize_diagnostics, diagnostics_to_feedback_text
@@ -54,6 +54,7 @@ class OrchestratorConfig:
     max_rounds_per_task: int = 12
     N_projections_base: int = 1
     N_projections_max: int = 3
+    candidates_per_projection: int = 1
     patch_size_limit_lines: int = 80
     # verifier
     max_verifier_seconds: int = 30
@@ -65,6 +66,9 @@ class OrchestratorConfig:
     rollback_threshold_LB: float = 0.6
     wilson_window_n: int = 30
     z_value: float = 1.96
+    # GMDT geometric projection (Algorithm 1)
+    tau: float = 0.0  # anti-alignment threshold for −i branching
+    force_minus_i_on_final_round: bool = True
     # patch minimizer
     enable_patch_minimizer: bool = False
     # reasoning models (o3, o1, etc.)
@@ -72,8 +76,19 @@ class OrchestratorConfig:
     # HARD2 hard caps (prevent context bloat / API stuck)
     max_llm_calls_per_task: int = 50
     max_total_verifier_calls_per_task: int = 50
+    feedback_format: str = "natural"
+    feedback_json_max_chars: int = 4500
+    feedback_max_failing_tests: int = 2
     max_prompt_chars: int = 120000
     max_log_chars_per_verifier: int = 20000
+    use_artifact_uembed: bool = False
+    embedding_backend: str = "hashed"
+    embedding_dim: int = 128
+    embedding_model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
+    embedding_projection_dim: int = 4
+    enable_vector_memory: bool = False
+    enable_faiss_memory: bool = False
+    memory_top_k: int = 2
 
 
 # ── Generator Client (OpenAI) ───────────────────────────────────────
@@ -124,8 +139,17 @@ class GeneratorClient:
         temperature: float = 0.4,
         seed: int = 0,
         ablation_mode: str = "A3",
+        feedback_format: str = "natural",
+        feedback_json_max_chars: int = 4500,
+        max_prompt_chars: int = 120000,
     ) -> Dict[str, Any]:
-        messages = self._build_messages(hint, ablation_mode=ablation_mode)
+        messages = self._build_messages(
+            hint,
+            ablation_mode=ablation_mode,
+            feedback_format=feedback_format,
+            feedback_json_max_chars=feedback_json_max_chars,
+            max_prompt_chars=max_prompt_chars,
+        )
         ck = self._cache_key(messages, seed, temperature, self.model)
         try:
             if ck in self._cache:
@@ -183,10 +207,37 @@ class GeneratorClient:
             logger.warning("Generator error: %s", e)
             return self._fallback_output(str(e))
 
+    @staticmethod
+    def _clip_text(text: str, max_chars: int, suffix: str = "\n... (truncated)") -> str:
+        text = str(text)
+        if max_chars <= 0 or len(text) <= max_chars:
+            return text
+        if max_chars <= len(suffix):
+            return text[:max_chars]
+        return text[: max_chars - len(suffix)] + suffix
+
+    @staticmethod
+    def _compact_feedback_text(text: str) -> str:
+        lines = [line for line in str(text).splitlines() if line.strip()]
+        if not lines:
+            return str(text)
+        compact = [lines[0]]
+        compact.extend([line for line in lines[1:] if "FAIL" in line][:2])
+        if len(compact) == 1:
+            compact.extend(lines[1:2])
+        return "\n".join(compact)
+
     def _build_messages(
-        self, hint: Dict[str, Any], *, ablation_mode: str
+        self,
+        hint: Dict[str, Any],
+        *,
+        ablation_mode: str,
+        feedback_format: str = "natural",
+        feedback_json_max_chars: int = 4500,
+        max_prompt_chars: int = 120000,
     ) -> List[Dict[str, str]]:
         inputs = hint.get("inputs", {})
+        feedback_format = str(feedback_format or "natural").strip().lower()
 
         system = (
             "You are an expert competitive programmer. "
@@ -206,17 +257,73 @@ class GeneratorClient:
             # State compression: truncate code if too long
             code_text = inputs["previous_code"]
             max_code = 8000
-            if len(code_text) > max_code:
-                code_text = code_text[:max_code] + "\n# ... (truncated)"
+            code_text = self._clip_text(code_text, max_code, "\n# ... (truncated)")
             user_parts.append(f"### Previous code:\n```python\n{code_text}\n```")
-        if inputs.get("diagnostics_text"):
+        diagnostics_text = inputs.get("diagnostics_text")
+        feedback_json = inputs.get("feedback_json")
+        include_diag_text = bool(diagnostics_text) and feedback_format in ("natural", "hybrid_json")
+        include_feedback_json = (
+            feedback_format in ("json", "hybrid_json")
+            and isinstance(feedback_json, dict)
+            and len(feedback_json) > 0
+        )
+
+        if include_diag_text:
             # State compression: truncate diagnostics
-            diag_text = inputs["diagnostics_text"]
+            diag_text = diagnostics_text
+            if feedback_format == "hybrid_json" and include_feedback_json:
+                diag_text = self._compact_feedback_text(diag_text)
             max_diag = 4000
-            if len(diag_text) > max_diag:
-                diag_text = diag_text[:max_diag] + "\n... (truncated)"
+            diag_text = self._clip_text(diag_text, max_diag)
             user_parts.append(f"### Test feedback:\n{diag_text}")
-        if ablation_mode == "A3" and inputs.get("projection_id"):
+
+        if include_feedback_json:
+            compact_json = json.dumps(
+                feedback_json,
+                ensure_ascii=False,
+                separators=(",", ":"),
+                sort_keys=False,
+            )
+            compact_json = self._clip_text(compact_json, max(0, int(feedback_json_max_chars)))
+            user_parts.append(
+                "### Structured feedback JSON (authoritative):\n"
+                f"{compact_json}"
+            )
+
+        if ablation_mode == "A3" and inputs.get("projection_analysis"):
+            # GMDT: render geometric analysis as concise natural language hint
+            pa = inputs["projection_analysis"]
+            ra = pa.get("rotation_analysis", {})
+            rec = pa.get("recommendation", "")
+            target_dim = ra.get("target_dimension", "")
+            target_failures = pa.get("target_failures", [])
+
+            hint_parts = []
+            if ra.get("rotation_applied"):
+                hint_parts.append(
+                    f"### Projection hint (geometric rotation, alignment={ra.get('alignment_score', 0):.2f}):"
+                )
+            else:
+                hint_parts.append("### Projection hint:")
+
+            if rec:
+                hint_parts.append(rec)
+
+            # Add specific test failure details for the target dimension
+            for tf in target_failures[:2]:
+                line = f"  FAIL {tf.get('test_id', '?')}"
+                if tf.get("input_preview"):
+                    inp_short = tf["input_preview"][:80]
+                    line += f": input={inp_short!r}"
+                if tf.get("expected"):
+                    line += f" expected={tf['expected']!r}"
+                if tf.get("actual"):
+                    line += f" got={tf['actual']!r}"
+                hint_parts.append(line)
+
+            user_parts.append("\n".join(hint_parts))
+        elif ablation_mode == "A3" and inputs.get("projection_id"):
+            # Fallback: legacy static hints (when no rotation analysis available)
             proj = inputs["projection_id"]
             if proj == "minus_i":
                 user_parts.append(
@@ -234,9 +341,16 @@ class GeneratorClient:
                     "Start from scratch with the simplest correct approach you can think of."
                 )
 
+        user_content = "\n\n".join(user_parts)
+        user_content = self._clip_text(
+            user_content,
+            max(0, int(max_prompt_chars)),
+            "\n\n[Prompt truncated due to max_prompt_chars]",
+        )
+
         return [
             {"role": "system", "content": system},
-            {"role": "user", "content": "\n\n".join(user_parts)},
+            {"role": "user", "content": user_content},
         ]
 
     def _normalize_output(self, raw: Dict[str, Any]) -> Dict[str, Any]:
@@ -294,6 +408,7 @@ def run_task(
     ablation_mode: str = "A3",
     seed_base: int = 42,
     starting_code: Optional[str] = None,
+    provenance: Optional[Dict[str, Any]] = None,
 ) -> Dict[str, Any]:
     """
     Execute a single task per Plan v2 Section 7 pseudocode.
@@ -318,15 +433,53 @@ def run_task(
     }
     latest_diag: Optional[Dict[str, Any]] = None
     latest_code: Optional[str] = None
+    latest_verifier_outputs: Optional[List[Dict[str, Any]]] = None
     score_prev: Optional[float] = None
     n_projections = cfg.N_projections_base
+    candidates_per_projection = max(1, int(getattr(cfg, "candidates_per_projection", 1)))
     recent_successes: List[bool] = []
+    v_history: List[List[float]] = []  # GMDT: failure vector history for Algorithm 1
+    u_history: List[List[float]] = []  # optional artifact-side embedding history
     tokens_total = 0
     verifier_calls_total = 0
     rounds_executed = 0
     stagnation_rounds = 0
     llm_calls_used = 0
 
+    artifact_embedder = None
+    if ablation_mode == "A3" and cfg.use_artifact_uembed:
+        try:
+            from .embeddings import ArtifactEmbedder
+
+            artifact_embedder = ArtifactEmbedder(
+                backend=cfg.embedding_backend,
+                dim=max(4, int(cfg.embedding_dim)),
+                model_name=cfg.embedding_model_name,
+            )
+            logger.info(
+                "    Artifact UEMBED enabled backend=%s dim=%d proj_dim=%d",
+                cfg.embedding_backend,
+                cfg.embedding_dim,
+                cfg.embedding_projection_dim,
+            )
+        except Exception as emb_err:
+            logger.warning(
+                "    Artifact UEMBED init failed; falling back to diagnostic proxy: %s",
+                emb_err,
+            )
+
+    def _compute_artifact_u(code_text: Optional[str]) -> Optional[List[float]]:
+        if artifact_embedder is None or not code_text:
+            return None
+        try:
+            return artifact_embedder.embed_projected(
+                code_text,
+                out_dim=max(1, int(cfg.embedding_projection_dim)),
+            )
+        except Exception as emb_err:
+            logger.warning("    Artifact UEMBED compute failed: %s", emb_err)
+            return None
+
     for t in range(cfg.max_rounds_per_task):
         # ── HARD CAP: stop if LLM call budget exhausted ──
         if llm_calls_used >= cfg.max_llm_calls_per_task:
@@ -367,8 +520,13 @@ def run_task(
             loss = decision.composite_loss
             latest_code = code
             latest_diag = diagnostics
+            latest_verifier_outputs = verifier_outputs
             score_prev = loss
             rounds_executed = 1
+            v_history.append(vembed(diagnostics, verifier_outputs))
+            u_vec = _compute_artifact_u(code)
+            if u_vec is not None:
+                u_history.append(u_vec)
 
             if decision.accepted:
                 best = {"solved": True, "loss": loss, "round": 0,
@@ -384,16 +542,61 @@ def run_task(
                     break
                 continue
 
+        # ── GMDT: Compute rotation analysis (Algorithm 1 lines 8–27) ──
+        rotation_result: Optional[Dict[str, Any]] = None
+        if ablation_mode == "A3" and latest_diag is not None and latest_verifier_outputs is not None:
+            force_minus_i = (
+                bool(cfg.force_minus_i_on_final_round)
+                and t == cfg.max_rounds_per_task - 1
+                and not latest_diag.get("hard_constraints_passed", False)
+            )
+            rotation_result = build_rotation_analysis(
+                v_t=v_history[-1] if v_history else vembed(latest_diag, latest_verifier_outputs),
+                v_history=v_history,
+                diagnostics=latest_diag,
+                verifier_outputs=latest_verifier_outputs,
+                tau=cfg.tau,
+                ablation_mode=ablation_mode,
+                u_t=u_history[-1] if u_history else None,
+                u_history=u_history,
+                force_minus_i=force_minus_i,
+            )
+            ev.log_event({
+                "type": "projection_analysis",
+                "round": t,
+                **rotation_result["meta"],
+            })
+
         # ── Determine active projections ──
         if ablation_mode in ("A0", "A1", "A2"):
             projections = ["base"]
+        elif rotation_result is not None:
+            # GMDT: use projections from rotation analysis (base + optionally minus_i)
+            projections = list(rotation_result["hints"].keys())
+            # Also include stagnation-driven expansion from legacy 4-cycle
+            if n_projections > len(projections):
+                legacy = enumerate_projections(n_projections, start_round=t)
+                for lp in legacy:
+                    if lp not in projections:
+                        projections.append(lp)
+                    if len(projections) >= n_projections:
+                        break
         else:
             projections = enumerate_projections(n_projections, start_round=t)
 
         round_candidates: List[Dict[str, Any]] = []
+        budget_exhausted = False
 
         for i, proj in enumerate(projections):
             # ── Build HintPacket ──
+            proj_hint_data = None
+            if rotation_result is not None and proj in rotation_result["hints"]:
+                proj_hint_data = rotation_result["hints"][proj]
+            if proj == "base":
+                # Keep base branch aligned with A2 feedback; reserve rotation
+                # payloads for non-base branches to avoid contaminating baseline.
+                proj_hint_data = None
+
             hint_pkt = build_hint_packet(
                 task_id=task_id,
                 round_id=t,
@@ -404,6 +607,8 @@ def run_task(
                 score_prev=score_prev,
                 ablation_mode=ablation_mode,
                 stagnation_rounds=stagnation_rounds,
+                rotation_analysis=proj_hint_data,
+                feedback_max_failing_tests=cfg.feedback_max_failing_tests,
             )
             hint_dict = hint_pkt.to_dict()
             ev.log_event({"type": "hint_packet", "round": t, "projection": proj, "hint": hint_dict})
@@ -412,75 +617,108 @@ def run_task(
             if ablation_mode == "A0" and t > 0:
                 break
 
-            # ── Call Generator ──
-            seed = seed_base + t * 10 + i
-            tokens_before = generator.total_tokens
-            out = generator.generate(
-                hint_dict,
-                temperature=temperature,
-                seed=seed,
-                ablation_mode=ablation_mode,
-            )
-            llm_calls_used += 1
-            tokens_total += generator.total_tokens - tokens_before
-            ev.log_event({"type": "round_output", "round": t, "projection": proj, "output": out})
-
-            code = out["delta_t_patch"]
-            x_hash = hash_text(code)
-
-            # ── Run Verifier Pack ──
-            verifier_outputs = verifier_runner.run_all(
-                code, test_cases, task_id=task_id
-            )
-            verifier_calls_total += len(verifier_outputs)
-            logger.info("    R%d  verifier done  proj=%s", t, proj)
-            ev.log_event({
-                "type": "verifier_outputs",
-                "round": t,
-                "projection": proj,
-                "outputs": verifier_outputs,
-            })
+            for sample_idx in range(candidates_per_projection):
+                if llm_calls_used >= cfg.max_llm_calls_per_task:
+                    logger.info(
+                        "    HARD CAP hit inside round: max_llm_calls_per_task=%d",
+                        cfg.max_llm_calls_per_task,
+                    )
+                    budget_exhausted = True
+                    break
+                if verifier_calls_total >= cfg.max_total_verifier_calls_per_task:
+                    logger.info(
+                        "    HARD CAP hit inside round: max_total_verifier_calls_per_task=%d",
+                        cfg.max_total_verifier_calls_per_task,
+                    )
+                    budget_exhausted = True
+                    break
 
-            # ── Critic: normalize diagnostics ──
-            diagnostics = normalize_diagnostics(verifier_outputs)
-            ev.log_event({
-                "type": "diagnostics",
-                "round": t,
-                "projection": proj,
-                "diagnostics": diagnostics,
-            })
+                sample_no = sample_idx + 1
+
+                # ── Call Generator ──
+                seed = seed_base + t * 1000 + i * 100 + sample_idx
+                tokens_before = generator.total_tokens
+                out = generator.generate(
+                    hint_dict,
+                    temperature=temperature,
+                    seed=seed,
+                    ablation_mode=ablation_mode,
+                    feedback_format=cfg.feedback_format,
+                    feedback_json_max_chars=cfg.feedback_json_max_chars,
+                    max_prompt_chars=cfg.max_prompt_chars,
+                )
+                llm_calls_used += 1
+                tokens_total += generator.total_tokens - tokens_before
+                ev.log_event({
+                    "type": "round_output",
+                    "round": t,
+                    "projection": proj,
+                    "sample": sample_no,
+                    "output": out,
+                })
 
-            # ── Arbiter: enforce hard constraints, compute composite loss ──
-            decision = arbitrate(
-                verifier_outputs,
-                ArbiterConfig(
-                    alpha={"V_lint": 0.1},
-                    hard_oracles=["V_tests"],
-                ),
-            )
+                code = out["delta_t_patch"]
+                x_hash = hash_text(code)
 
-            round_candidates.append({
-                "projection": proj,
-                "code": code,
-                "x_hash": x_hash,
-                "round_output": out,
-                "verifier_outputs": verifier_outputs,
-                "diagnostics": diagnostics,
-                "decision": decision,
-            })
+                # ── Run Verifier Pack ──
+                verifier_outputs = verifier_runner.run_all(
+                    code, test_cases, task_id=task_id
+                )
+                verifier_calls_total += len(verifier_outputs)
+                logger.info("    R%d  verifier done  proj=%s sample=%d", t, proj, sample_no)
+                ev.log_event({
+                    "type": "verifier_outputs",
+                    "round": t,
+                    "projection": proj,
+                    "sample": sample_no,
+                    "outputs": verifier_outputs,
+                })
+
+                # ── Critic: normalize diagnostics ──
+                diagnostics = normalize_diagnostics(verifier_outputs)
+                ev.log_event({
+                    "type": "diagnostics",
+                    "round": t,
+                    "projection": proj,
+                    "sample": sample_no,
+                    "diagnostics": diagnostics,
+                })
+
+                # ── Arbiter: enforce hard constraints, compute composite loss ──
+                decision = arbitrate(
+                    verifier_outputs,
+                    ArbiterConfig(
+                        alpha={"V_lint": 0.1},
+                        hard_oracles=["V_tests"],
+                    ),
+                )
 
-            ev.log_event({
-                "type": "candidate_eval",
-                "round": t,
-                "projection": proj,
-                "x_hash": x_hash,
-                "arbitration": {
-                    "accepted": decision.accepted,
-                    "composite_loss": decision.composite_loss,
-                    "hard_failed": decision.hard_failed,
-                    "rationale": decision.rationale,
-                },
-            })
+                round_candidates.append({
+                    "projection": proj,
+                    "sample": sample_no,
+                    "code": code,
+                    "x_hash": x_hash,
+                    "round_output": out,
+                    "verifier_outputs": verifier_outputs,
+                    "diagnostics": diagnostics,
+                    "decision": decision,
+                })
+
+                ev.log_event({
+                    "type": "candidate_eval",
+                    "round": t,
+                    "projection": proj,
+                    "sample": sample_no,
+                    "x_hash": x_hash,
+                    "arbitration": {
+                        "accepted": decision.accepted,
+                        "composite_loss": decision.composite_loss,
+                        "hard_failed": decision.hard_failed,
+                        "rationale": decision.rationale,
+                    },
+                })
+            if budget_exhausted:
+                break
 
         if not round_candidates:
             break
@@ -489,7 +727,14 @@ def run_task(
 
         # ── Select best candidate (tie-break by smaller patch size per Plan v2 §9) ──
         def _sort_key(c):
-            return (c["decision"].composite_loss, len(c["code"]))
+            nonbase_bonus = 0
+            if (
+                ablation_mode == "A3"
+                and c.get("projection") != "base"
+                and not c["decision"].accepted
+            ):
+                nonbase_bonus = -1
+            return (c["decision"].composite_loss, nonbase_bonus, len(c["code"]))
 
         accepted = [c for c in round_candidates if c["decision"].accepted]
         if accepted:
@@ -502,12 +747,18 @@ def run_task(
         loss = top["decision"].composite_loss
         latest_code = top["code"]
         latest_diag = top["diagnostics"]
+        latest_verifier_outputs = top["verifier_outputs"]
         score_prev = loss
+        v_history.append(vembed(latest_diag, latest_verifier_outputs))
+        u_vec = _compute_artifact_u(latest_code)
+        if u_vec is not None:
+            u_history.append(u_vec)
 
         ev.log_event({
             "type": "selection",
             "round": t,
             "chosen_projection": top["projection"],
+            "chosen_sample": top.get("sample"),
             "x_hash": top["x_hash"],
             "loss": loss,
             "hard_passed": top["decision"].accepted,
@@ -583,6 +834,7 @@ def run_task(
         final_decision=best,
         generator_id=cfg.model,
         seed=seed_base,
+        provenance=provenance,
     )
 
     return {
diff --git a/agice/projections.py b/agice/projections.py
index 19b7390..9965ece 100644
--- a/agice/projections.py
+++ b/agice/projections.py
@@ -7,17 +7,21 @@ Plan v2 maps v18's bounded J/(-i) operator cycle into prompt/template projection
 - neg_base
 - neg_minus_i
 
-This module provides deterministic projection ids and any prompt/template helpers.
+This module provides deterministic projection ids, geometric math helpers
+for the GMDT (−i) operator (Algorithm 1), and prompt/template helpers.
 """
 from __future__ import annotations
 
+import math
 from dataclasses import dataclass
-from typing import List, Literal
+from typing import Any, Dict, List, Literal, Optional, Tuple
 
 ProjectionId = Literal["base", "minus_i", "neg_base", "neg_minus_i"]
 
 CYCLE: List[ProjectionId] = ["base", "minus_i", "neg_base", "neg_minus_i"]
 
+DIMENSION_NAMES: List[str] = ["wrong_answer", "timeout", "crash", "syntax"]
+
 def projection_cycle(round_id: int) -> ProjectionId:
     """Deterministic 4-cycle projection id."""
     return CYCLE[round_id % len(CYCLE)]
@@ -28,3 +32,354 @@ def enumerate_projections(n: int, start_round: int) -> List[ProjectionId]:
     for i in range(n):
         out.append(projection_cycle(start_round + i))
     return out
+
+
+# ── GMDT Geometric Math (Algorithm 1) ─────────────────────────────────
+
+_EPS = 1e-12  # near-zero threshold
+
+
+def vec_norm(v: List[float]) -> float:
+    """Euclidean norm of v."""
+    return math.sqrt(sum(x * x for x in v))
+
+
+def vec_dot(a: List[float], b: List[float]) -> float:
+    """Dot product of a and b."""
+    return sum(ai * bi for ai, bi in zip(a, b))
+
+
+def vec_normalize(v: List[float]) -> List[float]:
+    """Unit vector in the direction of v. Returns zero vector if ‖v‖ ≈ 0."""
+    n = vec_norm(v)
+    if n < _EPS:
+        return [0.0] * len(v)
+    return [x / n for x in v]
+
+
+def _fit_dim(v: List[float], dim: int) -> List[float]:
+    if dim <= 0:
+        return []
+    if len(v) == dim:
+        return list(v)
+    if len(v) > dim:
+        return list(v[:dim])
+    return list(v) + [0.0] * (dim - len(v))
+
+
+def vembed(diagnostics: Dict[str, Any], verifier_outputs: List[Dict[str, Any]]) -> List[float]:
+    """VEMBED: decompose diagnostics into orthogonal failure vector v ∈ [0,1]⁴.
+
+    Dimensions: [wrong_answer_rate, timeout_rate, crash_rate, lint_loss].
+    Maps to Algorithm 1 line 10.
+
+    Reads per-test data from raw verifier_outputs (which have timed_out, exit_code)
+    rather than normalized diagnostics (which only has failing_tests summaries).
+    """
+    # Extract test_results from raw V_tests verifier output
+    test_results: List[Dict[str, Any]] = []
+    total = 1
+    for vo in verifier_outputs:
+        if vo.get("verifier_id") == "V_tests":
+            diag = vo.get("diagnostics", {})
+            test_results = diag.get("test_results", [])
+            total = max(diag.get("total", len(test_results)), 1)
+            break
+
+    wrong = sum(
+        1 for t in test_results
+        if not t.get("passed", True) and not t.get("timed_out", False) and t.get("exit_code", 0) == 0
+    )
+    timeout = sum(1 for t in test_results if t.get("timed_out", False))
+    crash = sum(
+        1 for t in test_results
+        if not t.get("passed", True) and not t.get("timed_out", False) and t.get("exit_code", 0) != 0
+    )
+
+    lint_loss = 0.0
+    for vo in verifier_outputs:
+        if vo.get("verifier_id") == "V_lint":
+            lint_loss = float(vo.get("loss", 0.0))
+
+    return [wrong / total, timeout / total, crash / total, lint_loss]
+
+
+def uembed_proxy(v_history: List[List[float]]) -> List[float]:
+    """PoC proxy for UEMBED: improvement direction Δ = v_{t-1} − v_t.
+
+    Positive values indicate improvement in that dimension.
+    Returns zero vector when history has fewer than 2 entries.
+    Maps to Algorithm 1 line 9 (proxy).
+    """
+    if len(v_history) < 2:
+        d = len(v_history[0]) if v_history else 4
+        return [0.0] * d
+    return [v_history[-2][i] - v_history[-1][i] for i in range(len(v_history[-1]))]
+
+
+def centroid(v_history: List[List[float]]) -> List[float]:
+    """Running centroid c_v = mean(v₀..v_{t-1}) over evidence history.
+
+    Maps to Algorithm 1 line 11.
+    """
+    if not v_history:
+        return [0.0, 0.0, 0.0, 0.0]
+    d = len(v_history[0])
+    n = len(v_history)
+    return [sum(v[i] for v in v_history) / n for i in range(d)]
+
+
+def compute_alignment(u_disp: List[float], v: List[float]) -> float:
+    """Cosine similarity ρ = <u_disp, r> / (‖u_disp‖ · ‖r‖) where r = normalize(v).
+
+    Returns 0.0 on degenerate inputs (zero vectors).
+    Maps to Algorithm 1 lines 13–15.
+    """
+    nu = vec_norm(u_disp)
+    nv = vec_norm(v)
+    if nu < _EPS or nv < _EPS:
+        return 0.0
+    return vec_dot(u_disp, v) / (nu * nv)
+
+
+def apply_j_operator(
+    u_disp: List[float], v_disp: List[float]
+) -> Tuple[List[float], List[float]]:
+    """Apply the complex-structure operator J[u;v] = [v; −u].
+
+    Returns (w_u_rotated, w_v_rotated).
+    Maps to Algorithm 1 line 19.
+    """
+    return (list(v_disp), [-x for x in u_disp])
+
+
+def _classify_tests_by_dimension(
+    verifier_outputs: List[Dict[str, Any]],
+) -> Dict[str, List[Dict[str, Any]]]:
+    """Group failing tests by failure dimension for targeted hints.
+
+    Reads from raw verifier_outputs (which have per-test timed_out, exit_code).
+    """
+    test_results: List[Dict[str, Any]] = []
+    for vo in verifier_outputs:
+        if vo.get("verifier_id") == "V_tests":
+            test_results = vo.get("diagnostics", {}).get("test_results", [])
+            break
+    groups: Dict[str, List[Dict[str, Any]]] = {
+        "wrong_answer": [],
+        "timeout": [],
+        "crash": [],
+    }
+    for t in test_results:
+        if t.get("passed", True):
+            continue
+        if t.get("timed_out", False):
+            groups["timeout"].append(t)
+        elif t.get("exit_code", 0) != 0:
+            groups["crash"].append(t)
+        else:
+            groups["wrong_answer"].append(t)
+    return groups
+
+
+def make_hint_data(
+    v_t: List[float],
+    w_u: List[float],
+    w_v: List[float],
+    verifier_outputs: List[Dict[str, Any]],
+    *,
+    tag: str,
+    rho: float = 0.0,
+) -> Dict[str, Any]:
+    """Build the structured JSON hint (MAKEHINT) for one projection branch.
+
+    Maps to Algorithm 1 lines 23–25.
+    """
+    d = len(v_t)
+    loss_dict = {DIMENSION_NAMES[i]: round(v_t[i], 4) for i in range(d)}
+
+    # Determine target dimension from rotated v-component
+    if tag == "minus_i":
+        # Use absolute values of w_v to find the dimension the J-rotation highlights
+        abs_wv = [abs(x) for x in w_v]
+        if max(abs_wv) < _EPS:
+            # w_v degenerate (stagnation: u_disp≈0 → J[0;v_disp]=[v_disp;0])
+            # Fall back to v_t: target the dominant failure dimension
+            target_idx = max(range(d), key=lambda i: (v_t[i], -i))
+        else:
+            target_idx = max(range(len(abs_wv)), key=lambda i: (abs_wv[i], -i))
+    else:
+        # base: use v_t directly — dominant failure dimension
+        target_idx = max(range(d), key=lambda i: (v_t[i], -i))
+
+    target_name = DIMENSION_NAMES[target_idx] if target_idx < len(DIMENSION_NAMES) else "unknown"
+
+    # Collect specific failing tests for the target dimension
+    groups = _classify_tests_by_dimension(verifier_outputs)
+    target_tests = groups.get(target_name, [])[:3]  # up to 3
+    target_failures = []
+    for ft in target_tests:
+        entry: Dict[str, Any] = {
+            "test_id": ft.get("test_id", "?"),
+            "failure_type": target_name,
+        }
+        inp = ft.get("input", "")
+        if inp:
+            entry["input_preview"] = inp[:120]
+        exp = ft.get("expected", "")
+        act = ft.get("actual", "")
+        if exp:
+            entry["expected"] = exp[:80]
+        if act:
+            entry["actual"] = act[:80]
+        if ft.get("stderr"):
+            entry["stderr_preview"] = ft["stderr"][:120]
+        target_failures.append(entry)
+
+    # Build recommendation text based on target dimension
+    rec = _build_recommendation(target_name, v_t, target_failures, tag, rho)
+
+    return {
+        "projection_id": tag,
+        "rotation_analysis": {
+            "alignment_score": round(rho, 4),
+            "rotation_applied": tag != "base",
+            "loss_vector": loss_dict,
+            "target_dimension": target_name,
+            "w_rotated": [round(x, 4) for x in w_u + w_v],
+        },
+        "target_failures": target_failures,
+        "recommendation": rec,
+    }
+
+
+def _build_recommendation(
+    target_name: str,
+    v_t: List[float],
+    target_failures: List[Dict[str, Any]],
+    tag: str,
+    rho: float,
+) -> str:
+    """Generate a natural-language recommendation for the target dimension."""
+    n_fails = len(target_failures)
+    parts: List[str] = []
+
+    if tag == "minus_i":
+        parts.append(
+            f"Geometric rotation (alignment={rho:.2f}) indicates the dimension "
+            f"'{target_name}' is not being addressed by your current improvement direction."
+        )
+
+    if target_name == "wrong_answer":
+        parts.append(
+            f"{n_fails} test(s) produce incorrect output. "
+            "Focus on algorithm correctness — check your core logic, "
+            "formula, or data structure choice."
+        )
+    elif target_name == "timeout":
+        parts.append(
+            f"{n_fails} test(s) timed out. "
+            "Your solution's time complexity is too high. "
+            "Consider O(n log n) approaches: binary search, segment tree, "
+            "monotonic stack, or two-pointer technique."
+        )
+    elif target_name == "crash":
+        parts.append(
+            f"{n_fails} test(s) crashed with runtime errors. "
+            "Check for edge cases: empty input, single element, integer overflow, "
+            "division by zero, or index out of bounds."
+        )
+    elif target_name == "syntax":
+        parts.append(
+            "Your code has syntax errors preventing execution. "
+            "Fix syntax before optimizing the algorithm."
+        )
+    else:
+        parts.append(f"Focus on the '{target_name}' failure dimension.")
+
+    return " ".join(parts)
+
+
+def build_rotation_analysis(
+    v_t: List[float],
+    v_history: List[List[float]],
+    diagnostics: Dict[str, Any],
+    verifier_outputs: List[Dict[str, Any]],
+    *,
+    tau: float = 0.0,
+    ablation_mode: str = "A3",
+    u_t: Optional[List[float]] = None,
+    u_history: Optional[List[List[float]]] = None,
+    allow_degenerate_rotation: bool = False,
+    force_minus_i: bool = False,
+) -> Dict[str, Any]:
+    """Top-level function: compute full rotation analysis for one round.
+
+    Returns a dict keyed by projection_id ("base", optionally "minus_i")
+    containing the structured JSON hint data for each branch.
+    Also returns metadata for evidence logging.
+    """
+    d = len(v_t)
+
+    # Step 1: u displacement (artifact embedder when available, else diagnostic proxy)
+    u_source = "diagnostic_proxy"
+    if u_t is not None:
+        u_cur = _fit_dim(u_t, d)
+        if u_history:
+            u_hist = [_fit_dim(u, d) for u in u_history]
+            c_u = centroid(u_hist[:-1]) if len(u_hist) > 1 else [0.0] * d
+            u_disp = [u_cur[i] - c_u[i] for i in range(d)]
+        else:
+            u_disp = u_cur
+        u_source = "artifact_embedder"
+    elif u_history:
+        u_hist = [_fit_dim(u, d) for u in u_history]
+        u_cur = u_hist[-1]
+        c_u = centroid(u_hist[:-1]) if len(u_hist) > 1 else [0.0] * d
+        u_disp = [u_cur[i] - c_u[i] for i in range(d)]
+        u_source = "artifact_embedder"
+    else:
+        u_disp = uembed_proxy(v_history)
+
+    # Step 2: centroid (exclude current v_t — it's the last entry)
+    c_v = centroid(v_history[:-1]) if len(v_history) > 1 else [0.0] * d
+
+    # Step 3: v displacement from centroid
+    v_disp = [v_t[i] - c_v[i] for i in range(d)]
+
+    # Step 4: alignment
+    rho = compute_alignment(u_disp, v_t)
+
+    # Step 5: build projection set P and hints
+    hints: Dict[str, Dict[str, Any]] = {}
+    hints["base"] = make_hint_data(v_t, u_disp, v_disp, verifier_outputs, tag="base", rho=rho)
+
+    rotation_applied = False
+    u_degenerate = vec_norm(u_disp) < _EPS
+    has_history = len(v_history) >= 2
+    # Trigger rotation on anti-alignment OR degenerate u_disp with history
+    # (stagnation makes u_disp→0, but that's exactly when rotation is needed)
+    should_rotate = (rho < -tau) or (allow_degenerate_rotation and u_degenerate and has_history)
+    if (should_rotate or force_minus_i) and ablation_mode == "A3":
+        w_u_rot, w_v_rot = apply_j_operator(u_disp, v_disp)
+        hints["minus_i"] = make_hint_data(
+            v_t, w_u_rot, w_v_rot, verifier_outputs, tag="minus_i", rho=rho
+        )
+        rotation_applied = True
+
+    # Metadata for evidence logging (Algorithm 1 line 27)
+    meta = {
+        "v_t": [round(x, 4) for x in v_t],
+        "u_disp": [round(x, 4) for x in u_disp],
+        "c_v": [round(x, 4) for x in c_v],
+        "rho": round(rho, 4),
+        "tau": tau,
+        "norm_u_disp": round(vec_norm(u_disp), 4),
+        "norm_v_disp": round(vec_norm(v_disp), 4),
+        "P": list(hints.keys()),
+        "rotation_applied": rotation_applied,
+        "u_source": u_source,
+        "force_minus_i": bool(force_minus_i),
+    }
+
+    return {"hints": hints, "meta": meta}
diff --git a/bench/run_ablation.py b/bench/run_ablation.py
index 59236e7..100496a 100644
--- a/bench/run_ablation.py
+++ b/bench/run_ablation.py
@@ -19,6 +19,7 @@ Metrics (Plan v2 Section 13):
 from __future__ import annotations
 
 import argparse
+import inspect
 import json
 import logging
 import os
@@ -28,8 +29,9 @@ from typing import Any, Dict, List, Optional
 import yaml
 
 from agice.orchestrator import OrchestratorConfig, GeneratorClient, run_task
+from agice.provenance import sha256_file
 from verifier.runner import VerifierRunner, VerifierRunnerConfig
-from .livecodebench import load_slice, format_lcb_output
+from .task_loader import load_tasks, format_eval_output
 
 logging.basicConfig(
     level=logging.INFO,
@@ -107,6 +109,7 @@ def main(argv=None) -> int:
     p = argparse.ArgumentParser(prog="agice-ablation")
     p.add_argument("--config", required=True)
     p.add_argument("--slice", required=True)
+    p.add_argument("--task_family", default="livecodebench", choices=["livecodebench", "swebench"])
     p.add_argument("--out_dir", default="runs")
     p.add_argument("--max_tasks", type=int, default=10)
     p.add_argument("--ablations", nargs="*", default=ABLATIONS,
@@ -122,20 +125,40 @@ def main(argv=None) -> int:
     vr = VerifierRunner(VerifierRunnerConfig(
         max_seconds=int(cfg_raw.get("max_verifier_seconds", 30)),
         enable_lint_oracle=bool(cfg_raw.get("enable_lint_oracle", False)),
+        enable_ruff_oracle=bool(cfg_raw.get("enable_ruff_oracle", False)),
+        enable_pyright_oracle=bool(cfg_raw.get("enable_pyright_oracle", False)),
+        enable_mypy_oracle=bool(cfg_raw.get("enable_mypy_oracle", False)),
+        enable_hypothesis_oracle=bool(cfg_raw.get("enable_hypothesis_oracle", False)),
+        static_oracle_timeout_seconds=int(cfg_raw.get("static_oracle_timeout_seconds", 20)),
+        strict_external_oracles=bool(cfg_raw.get("strict_external_oracles", False)),
+        hypothesis_examples=int(cfg_raw.get("hypothesis_examples", 12)),
+        ruff_bin=str(cfg_raw.get("ruff_bin", "ruff")),
+        pyright_bin=str(cfg_raw.get("pyright_bin", "pyright")),
+        mypy_bin=str(cfg_raw.get("mypy_bin", "mypy")),
     ))
 
-    tasks = load_slice(args.slice)[: args.max_tasks]
+    tasks = load_tasks(args.task_family, args.slice)[: args.max_tasks]
     logger.info("Loaded %d tasks from slice '%s'", len(tasks), args.slice)
 
     run_id = time.strftime("%Y%m%d_%H%M%S")
     run_dir = os.path.join(args.out_dir, run_id)
     os.makedirs(run_dir, exist_ok=True)
+    os.environ["AGICE_RUN_ID"] = run_id
+
+    if args.task_family == "livecodebench":
+        slice_path = os.path.join("cache", "livecodebench", "slices", f"{args.slice}.json")
+    else:
+        slice_path = os.path.join("cache", "swebench", "slices", f"{args.slice}.json")
 
     summary: Dict[str, Any] = {
         "run_id": run_id,
         "slice": args.slice,
+        "task_family": args.task_family,
+        "slice_sha256": sha256_file(slice_path),
         "model": model,
         "seed": seed,
+        "config_path": args.config,
+        "config_sha256": sha256_file(args.config),
         "ablation": {},
     }
 
@@ -160,28 +183,56 @@ def main(argv=None) -> int:
         os.makedirs(a_dir, exist_ok=True)
 
         results: List[Dict[str, Any]] = []
+        run_task_params = set(inspect.signature(run_task).parameters.keys())
+        evaluated_task_ids = [t.task_id for t in tasks]
+        run_provenance_base = {
+            "run_id": run_id,
+            "slice_id": args.slice,
+            "task_family": args.task_family,
+            "slice_sha256": summary.get("slice_sha256"),
+            "evaluated_task_ids": evaluated_task_ids,
+            "seed_base": seed,
+            "config_path": args.config,
+            "config_sha256": summary.get("config_sha256"),
+            "model": model,
+            "reasoning_effort": reasoning_effort,
+        }
         for ti, task in enumerate(tasks):
             logger.info("  [%s] Task %d/%d: %s", abl, ti + 1, len(tasks), task.task_id)
-            test_cases = task.get_test_cases(include_private=True)
+            include_private = bool(args.task_family == "livecodebench")
+            test_cases = task.get_test_cases(include_private=include_private)
             starting_code = task.metadata.get("starting_code", None)
+            run_kwargs: Dict[str, Any] = {
+                "task_id": task.task_id,
+                "prompt": task.prompt,
+                "test_cases": test_cases,
+                "run_dir": a_dir,
+                "cfg": ocfg,
+                "generator": gen,
+                "verifier_runner": vr,
+                "ablation_mode": abl,
+                "seed_base": seed,
+                "starting_code": starting_code,
+            }
+            if "provenance" in run_task_params:
+                run_kwargs["provenance"] = {
+                    **run_provenance_base,
+                    "task_index": ti,
+                    "task_count": len(tasks),
+                    "task_manifest": {
+                        "repo_url": task.repo_url,
+                        "commit": task.commit,
+                    },
+                }
             r = run_task(
-                task_id=task.task_id,
-                prompt=task.prompt,
-                test_cases=test_cases,
-                run_dir=a_dir,
-                cfg=ocfg,
-                generator=gen,
-                verifier_runner=vr,
-                ablation_mode=abl,
-                seed_base=seed,
-                starting_code=starting_code,
+                **run_kwargs,
             )
             results.append(r)
             status = "✓" if r["solved"] else "✗"
             logger.info("    %s rounds=%d solved=%s", status, r["rounds_used"], r["solved"])
 
         metrics = compute_metrics(results)
-        lcb_output = format_lcb_output(results)
+        lcb_output = format_eval_output(results, args.task_family)
 
         summary["ablation"][abl] = {
             "display_name": ABLATION_DISPLAY_NAMES.get(abl, abl),
diff --git a/configs/default.yaml b/configs/default.yaml
index ca71473..fa0e27e 100644
--- a/configs/default.yaml
+++ b/configs/default.yaml
@@ -10,15 +10,28 @@ temperature_safe_mode: 0.2
 max_rounds_per_task: 12
 N_projections_base: 1
 N_projections_max: 3
+candidates_per_projection: 2
 patch_size_limit_lines: 80
+patch_size_limit_structural_nodes: 250
 
 # --- Verifier ---
 max_verifier_seconds: 30
-enable_lint_oracle: false          # disabled for code-gen PoC
+enable_lint_oracle: true
+enable_ruff_oracle: true
+enable_pyright_oracle: true
+enable_mypy_oracle: false
+enable_hypothesis_oracle: true
+static_oracle_timeout_seconds: 20
+strict_external_oracles: false
+hypothesis_examples: 12
+ruff_bin: "ruff"
+pyright_bin: "pyright"
+mypy_bin: "mypy"
 
 # --- Convergence ---
 stagnation_window_m: 3
 eps_improvement: 1.0e-6
+loss_tie_epsilon: 1.0e-12
 
 # --- Governance (Wilson LB gate) ---
 rollback_threshold_LB: 0.6
@@ -27,3 +40,18 @@ z_value: 1.96
 
 # --- Patch minimizer ---
 enable_patch_minimizer: false      # enable after hard-pass for ddmin
+
+# --- MA −i (multi-axis) ---
+ma_minus_i_max_axes: 2
+
+# --- Drop 2 feedback and embedding controls ---
+feedback_format: "hybrid_json"
+feedback_json_max_chars: 6000
+use_artifact_uembed: true
+embedding_backend: "hashed"
+embedding_dim: 128
+embedding_model_name: "sentence-transformers/all-MiniLM-L6-v2"
+embedding_projection_dim: 4
+enable_vector_memory: true
+enable_faiss_memory: true
+memory_top_k: 2
diff --git a/evidence/bundle.py b/evidence/bundle.py
index 3adfc18..6066133 100644
--- a/evidence/bundle.py
+++ b/evidence/bundle.py
@@ -12,6 +12,7 @@ from __future__ import annotations
 
 import json
 import os
+import tempfile
 import time
 import uuid
 from typing import Any, Dict, List, Optional
@@ -49,6 +50,8 @@ class EvidenceBundleWriter:
 
         with open(self.events_path, "a", encoding="utf-8") as f:
             f.write(json.dumps(event, ensure_ascii=False, default=str) + "\n")
+            f.flush()
+            os.fsync(f.fileno())
 
         self._prev_hash = event_hash
         self._events_count += 1
@@ -57,6 +60,7 @@ class EvidenceBundleWriter:
             self._round_summaries.append({
                 "round": event.get("round"),
                 "x_hash": event.get("x_hash"),
+                "x_canonical_hash": event.get("x_canonical_hash"),
                 "loss": event.get("loss"),
                 "hard_passed": event.get("hard_passed"),
                 "projection": event.get("chosen_projection"),
@@ -64,16 +68,49 @@ class EvidenceBundleWriter:
 
     def store_artifact(self, filename: str, content: str) -> str:
         path = os.path.join(self.bundle_dir, "artifacts", filename)
-        with open(path, "w", encoding="utf-8") as f:
-            f.write(content)
+        tmp_dir = os.path.dirname(path)
+        fd, tmp_path = tempfile.mkstemp(prefix=os.path.basename(path) + ".", suffix=".tmp", dir=tmp_dir)
+        try:
+            with os.fdopen(fd, "w", encoding="utf-8") as f:
+                f.write(content)
+                f.flush()
+                os.fsync(f.fileno())
+            os.replace(tmp_path, path)
+        finally:
+            try:
+                if os.path.exists(tmp_path):
+                    os.unlink(tmp_path)
+            except OSError:
+                pass
         return path
 
+    @staticmethod
+    def _atomic_write_json(path: str, obj: Dict[str, Any]) -> None:
+        tmp_dir = os.path.dirname(path)
+        fd, tmp_path = tempfile.mkstemp(prefix=os.path.basename(path) + ".", suffix=".tmp", dir=tmp_dir)
+        try:
+            with os.fdopen(fd, "w", encoding="utf-8") as f:
+                json.dump(obj, f, ensure_ascii=False, indent=2, sort_keys=True, default=str)
+                f.flush()
+                os.fsync(f.fileno())
+            os.replace(tmp_path, path)
+        finally:
+            try:
+                if os.path.exists(tmp_path):
+                    os.unlink(tmp_path)
+            except OSError:
+                pass
+
     def finalize(
         self,
         environment: Dict[str, Any],
         final_decision: Dict[str, Any],
         generator_id: str = "unknown",
         seed: int = 0,
+        *,
+        config_id: Optional[str] = None,
+        environment_hash: Optional[str] = None,
+        provenance: Optional[Dict[str, Any]] = None,
     ) -> Dict[str, Any]:
         manifest = {
             "bundle_id": self.bundle_id,
@@ -89,11 +126,19 @@ class EvidenceBundleWriter:
                 "events_count": self._events_count,
             },
         }
+        if config_id is not None:
+            manifest["config_id"] = config_id
+        if environment_hash is not None:
+            manifest["environment_hash"] = environment_hash
+        if provenance is not None:
+            manifest["provenance"] = provenance
         validate_manifest(manifest)
 
         manifest_path = os.path.join(self.bundle_dir, "bundle_manifest.json")
-        with open(manifest_path, "w", encoding="utf-8") as f:
-            json.dump(manifest, f, ensure_ascii=False, indent=2, sort_keys=True)
+        self._atomic_write_json(manifest_path, manifest)
+        # Validate on write (guards against partial/corrupt JSON)
+        with open(manifest_path, "r", encoding="utf-8") as f:
+            validate_manifest(json.load(f))
 
         integrity = {
             "bundle_id": self.bundle_id,
@@ -107,8 +152,10 @@ class EvidenceBundleWriter:
             "signature": None,
             "log_anchor": None,
         }
-        with open(os.path.join(self.bundle_dir, "integrity.json"), "w", encoding="utf-8") as f:
-            json.dump(integrity, f, ensure_ascii=False, indent=2, sort_keys=True)
+        integrity_path = os.path.join(self.bundle_dir, "integrity.json")
+        self._atomic_write_json(integrity_path, integrity)
+        with open(integrity_path, "r", encoding="utf-8") as f:
+            json.load(f)  # parse validation
 
         replay = {
             "bundle_dir": self.bundle_dir,
@@ -123,7 +170,9 @@ class EvidenceBundleWriter:
                 "PYTHONHASHSEED": "0",
             },
         }
-        with open(os.path.join(self.bundle_dir, "replay.json"), "w", encoding="utf-8") as f:
-            json.dump(replay, f, ensure_ascii=False, indent=2, sort_keys=True)
+        replay_path = os.path.join(self.bundle_dir, "replay.json")
+        self._atomic_write_json(replay_path, replay)
+        with open(replay_path, "r", encoding="utf-8") as f:
+            json.load(f)  # parse validation
 
         return manifest
diff --git a/evidence/replay.py b/evidence/replay.py
index 533d0c2..dc5f708 100644
--- a/evidence/replay.py
+++ b/evidence/replay.py
@@ -4,8 +4,8 @@ Deterministic replay tool.
 Plan v2 Section 10:
 - Load evidence bundle
 - Verify hash chain integrity of events.jsonl
-- Re-run verifiers on stored code artifacts
-- Compare outcomes
+- Re-run verifiers on stored selected code artifacts
+- Compare outcomes and classify drift
 - Emit replay_report.json
 """
 from __future__ import annotations
@@ -13,9 +13,14 @@ from __future__ import annotations
 import argparse
 import json
 import os
+import tempfile
+import sys
 from typing import Any, Dict, List
 
 from evidence.integrity import hash_event, hash_file, sha256_hex
+from agice.canonicalize import hash_text
+from bench.livecodebench import load_slice
+from verifier.runner import VerifierRunner, VerifierRunnerConfig
 
 
 def verify_hash_chain(events_path: str) -> Dict[str, Any]:
@@ -53,8 +58,94 @@ def verify_hash_chain(events_path: str) -> Dict[str, Any]:
     return {"valid": True, "count": count, "tip": prev_hash}
 
 
+def _atomic_write_json(path: str, obj: Dict[str, Any]) -> None:
+    tmp_dir = os.path.dirname(path)
+    fd, tmp_path = tempfile.mkstemp(prefix=os.path.basename(path) + ".", suffix=".tmp", dir=tmp_dir)
+    try:
+        with os.fdopen(fd, "w", encoding="utf-8") as f:
+            json.dump(obj, f, ensure_ascii=False, indent=2, sort_keys=True, default=str)
+            f.flush()
+            os.fsync(f.fileno())
+        os.replace(tmp_path, path)
+    finally:
+        try:
+            if os.path.exists(tmp_path):
+                os.unlink(tmp_path)
+        except OSError:
+            pass
+
+
+def _find_repo_root(start_dir: str) -> str:
+    d = os.path.abspath(start_dir)
+    while True:
+        if os.path.isdir(os.path.join(d, "agice")) and os.path.exists(os.path.join(d, "pyproject.toml")):
+            return d
+        parent = os.path.dirname(d)
+        if parent == d:
+            return os.path.abspath(start_dir)
+        d = parent
+
+
+def _summarize_verifier_outputs(outs: List[Dict[str, Any]]) -> Dict[str, Any]:
+    by_id: Dict[str, Any] = {}
+    for o in outs:
+        vid = o.get("verifier_id", "unknown")
+        entry: Dict[str, Any] = {
+            "passed": bool(o.get("passed", False)),
+            "loss": float(o.get("loss", 0.0)),
+        }
+        diag = o.get("diagnostics", {}) if isinstance(o.get("diagnostics"), dict) else {}
+        if vid == "V_tests":
+            tr = diag.get("test_results", []) if isinstance(diag.get("test_results"), list) else []
+            failing_ids = [t.get("test_id") for t in tr if not t.get("passed", True)]
+            entry["tests"] = {
+                "total": int(diag.get("total", len(tr))),
+                "passed": int(diag.get("passed", 0)),
+                "failed": int(diag.get("failed", max(int(diag.get("total", len(tr))) - int(diag.get("passed", 0)), 0))),
+                "failing_test_ids": failing_ids[:20],
+            }
+        by_id[vid] = entry
+    return by_id
+
+
+def _extract_events(events_path: str) -> List[Dict[str, Any]]:
+    events: List[Dict[str, Any]] = []
+    if not os.path.exists(events_path):
+        return events
+    with open(events_path, "r", encoding="utf-8") as f:
+        for line in f:
+            line = line.strip()
+            if not line:
+                continue
+            events.append(json.loads(line))
+    return events
+
+
+def _load_selected_code(bundle_dir: str, events: List[Dict[str, Any]], *, round_id: int, projection: str, x_hash: str) -> Dict[str, Any]:
+    artifacts_path = os.path.join(bundle_dir, "artifacts", f"selected_round_{round_id}.py")
+    if os.path.exists(artifacts_path):
+        with open(artifacts_path, "r", encoding="utf-8") as f:
+            code = f.read()
+        return {"ok": True, "source": "artifacts/selected_round", "code": code}
+
+    # Fallback: search round_output events for this (round, projection) and match x_hash.
+    for ev in events:
+        if ev.get("type") != "round_output":
+            continue
+        if int(ev.get("round", -1)) != int(round_id):
+            continue
+        if str(ev.get("projection", "")) != str(projection):
+            continue
+        out = ev.get("output", {})
+        if isinstance(out, dict):
+            code = out.get("delta_t_patch", "")
+            if isinstance(code, str) and hash_text(code) == x_hash:
+                return {"ok": True, "source": "events.round_output", "code": code}
+    return {"ok": False, "error": "selected code not found in artifacts or events"}
+
+
 def replay_bundle(bundle_dir: str) -> Dict[str, Any]:
-    """Replay a single evidence bundle: verify hash chain + file integrity."""
+    """Replay a single evidence bundle: verify integrity + re-run verifiers for selected artifacts."""
     manifest_path = os.path.join(bundle_dir, "bundle_manifest.json")
     events_path = os.path.join(bundle_dir, "events.jsonl")
     integrity_path = os.path.join(bundle_dir, "integrity.json")
@@ -71,6 +162,7 @@ def replay_bundle(bundle_dir: str) -> Dict[str, Any]:
         manifest = json.load(f)
     report["task_id"] = manifest.get("task_id")
     report["bundle_id"] = manifest.get("bundle_id")
+    provenance = manifest.get("provenance", {}) if isinstance(manifest.get("provenance"), dict) else {}
 
     # 2. Verify hash chain
     chain_result = verify_hash_chain(events_path)
@@ -95,11 +187,128 @@ def replay_bundle(bundle_dir: str) -> Dict[str, Any]:
     if chain_result.get("valid") and manifest_tip:
         report["checks"]["chain_tip_match"] = chain_result.get("tip") == manifest_tip
 
-    all_ok = (
+    integrity_ok = (
         chain_result.get("valid", False)
         and report["checks"].get("events_file_hash", {}).get("match", False)
         and report["checks"].get("chain_tip_match", True)
     )
+
+    # 5. Re-run verifiers for selected artifacts (when provenance is sufficient)
+    events = _extract_events(events_path)
+    selections = [
+        e for e in events
+        if e.get("type") == "selection"
+    ]
+    recorded_vo = {
+        (int(e.get("round", -1)), str(e.get("projection", ""))): e.get("outputs", [])
+        for e in events
+        if e.get("type") == "verifier_outputs"
+    }
+
+    replay_section: Dict[str, Any] = {"attempted": False, "rounds": []}
+    slice_id = provenance.get("slice_id")
+    if isinstance(slice_id, str) and slice_id:
+        repo_root = _find_repo_root(bundle_dir)
+        cache_dir = os.path.join(repo_root, "cache", "livecodebench")
+        slice_path = os.path.join(cache_dir, "slices", f"{slice_id}.json")
+        expected_slice_sha = provenance.get("slice_sha256")
+        actual_slice_sha = None
+        try:
+            import hashlib
+            h = hashlib.sha256()
+            with open(slice_path, "rb") as f:
+                for chunk in iter(lambda: f.read(1024 * 1024), b""):
+                    h.update(chunk)
+            actual_slice_sha = h.hexdigest()
+        except OSError:
+            actual_slice_sha = None
+
+        replay_section.update({
+            "attempted": True,
+            "repo_root": repo_root,
+            "slice_id": slice_id,
+            "slice_path": slice_path,
+            "slice_sha256_expected": expected_slice_sha,
+            "slice_sha256_actual": actual_slice_sha,
+            "slice_sha256_match": (expected_slice_sha == actual_slice_sha) if expected_slice_sha and actual_slice_sha else None,
+        })
+
+        try:
+            tasks = load_slice(slice_id, cache_dir=cache_dir)
+            task = next((t for t in tasks if t.task_id == report.get("task_id")), None)
+            if task is None:
+                replay_section["error"] = "task_id not found in slice"
+            else:
+                test_cases = task.get_test_cases(include_private=True)
+                vcfg = provenance.get("verifier_runner_cfg", {}) if isinstance(provenance.get("verifier_runner_cfg"), dict) else {}
+                vr = VerifierRunner(VerifierRunnerConfig(
+                    max_seconds=int(vcfg.get("max_seconds", 30)),
+                    enable_lint_oracle=bool(vcfg.get("enable_lint_oracle", False)),
+                    python_bin=str(vcfg.get("python_bin") or sys.executable),
+                ))
+                replay_section["verifier_runner_cfg"] = {
+                    "max_seconds": vr.cfg.max_seconds,
+                    "enable_lint_oracle": vr.cfg.enable_lint_oracle,
+                    "python_bin": vr.cfg.python_bin,
+                }
+
+                for sel in selections:
+                    rid = int(sel.get("round", -1))
+                    proj = str(sel.get("chosen_projection", ""))
+                    if proj == "carry":
+                        replay_section["rounds"].append({
+                            "round": rid,
+                            "projection": proj,
+                            "status": "SKIP",
+                            "reason": "carry selection has no new artifact to replay",
+                        })
+                        continue
+                    xh = str(sel.get("x_hash", ""))
+                    code_res = _load_selected_code(bundle_dir, events, round_id=rid, projection=proj, x_hash=xh)
+                    if not code_res.get("ok"):
+                        replay_section["rounds"].append({
+                            "round": rid,
+                            "projection": proj,
+                            "x_hash": xh,
+                            "status": "FAIL",
+                            "error": code_res.get("error"),
+                        })
+                        continue
+
+                    code = code_res["code"]
+                    replay_outs = vr.run_all(code, test_cases, task_id=report.get("task_id", ""))
+                    recorded = recorded_vo.get((rid, proj), [])
+                    rec_sum = _summarize_verifier_outputs(recorded) if isinstance(recorded, list) else {}
+                    rep_sum = _summarize_verifier_outputs(replay_outs)
+                    match = rec_sum == rep_sum
+                    replay_section["rounds"].append({
+                        "round": rid,
+                        "projection": proj,
+                        "x_hash": xh,
+                        "code_source": code_res.get("source"),
+                        "match": match,
+                        "recorded": rec_sum,
+                        "replayed": rep_sum,
+                    })
+        except Exception as e:
+            replay_section["error"] = str(e)
+    else:
+        replay_section["attempted"] = False
+        replay_section["reason"] = "slice_id missing from manifest provenance"
+
+    report["checks"]["verifier_replay"] = replay_section
+    replay_ok = True
+    if replay_section.get("attempted"):
+        if replay_section.get("slice_sha256_match") is False:
+            replay_ok = False
+        # If we attempted replay, require all non-SKIP rounds to match.
+        for r in replay_section.get("rounds", []):
+            if r.get("status") in ("FAIL",):
+                replay_ok = False
+            if r.get("match") is False:
+                replay_ok = False
+
+    all_ok = bool(integrity_ok and replay_ok)
     report["status"] = "PASS" if all_ok else "FAIL"
     return report
 
@@ -125,8 +334,7 @@ def main(argv=None) -> int:
     for bd in targets:
         report = replay_bundle(bd)
         reports.append(report)
-        with open(os.path.join(bd, "replay_report.json"), "w", encoding="utf-8") as f:
-            json.dump(report, f, ensure_ascii=False, indent=2, sort_keys=True)
+        _atomic_write_json(os.path.join(bd, "replay_report.json"), report)
         status = report.get("status", "FAIL")
         if status == "PASS":
             passed += 1
@@ -139,8 +347,7 @@ def main(argv=None) -> int:
         "reports": reports,
     }
     out_path = os.path.join(args.bundle_dir, "replay_batch_report.json")
-    with open(out_path, "w", encoding="utf-8") as f:
-        json.dump(batch, f, ensure_ascii=False, indent=2, sort_keys=True)
+    _atomic_write_json(out_path, batch)
 
     print(f"\nReplay: {passed}/{len(reports)} passed")
     print(f"Wrote {out_path}")
diff --git a/pyproject.toml b/pyproject.toml
index ae16213..21cfce0 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -8,3 +8,6 @@ description = "AGICE Reasoning Model — verification-driven convergence for cod
 agice = "agice.cli:main"
 agice-ablation = "bench.run_ablation:main"
 agice-replay = "evidence.replay:main"
+agice-demo-replay = "evidence.demo_replay:main"
+agice-executive-csv = "evidence.executive_csv:main"
+agice-zenodo-bundle = "evidence.zenodo_bundle:main"
diff --git a/requirements.txt b/requirements.txt
index 7fa94f6..ea82e01 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -5,6 +5,7 @@ PyYAML>=6.0,<7.0
 unidiff>=0.7,<1.0
 openai>=1.30,<2.0
 tiktoken>=0.7,<1.0
+python-dotenv>=1.0,<2.0
 
 # Verifier (container-based)
 docker>=7.0,<8.0
@@ -12,6 +13,15 @@ docker>=7.0,<8.0
 # optional: patch minimizer
 # ddmin>=0.1
 
+# Drop 2 optional integrations (P0/P1)
+hypothesis>=6.0,<7.0
+libcst>=1.0,<2.0
+mypy>=1.0,<2.0
+pyright>=1.1,<2.0
+ruff>=0.7,<1.0
+sentence-transformers>=3.0,<4.0
+faiss-cpu>=1.8,<2.0
+
 # optional observability
 # opentelemetry-api>=1.0
 # opentelemetry-sdk>=1.0
diff --git a/verifier/runner.py b/verifier/runner.py
index 629751c..c700473 100644
--- a/verifier/runner.py
+++ b/verifier/runner.py
@@ -1,20 +1,20 @@
 """
-VerifierRunner: runs the verifier pack for code-generation tasks.
+VerifierRunner: executes the verifier pack for code-generation tasks.
 
-Plan v2 Section 9:
-- V_tests (hard): run generated code against test cases; output structured diagnostics.
-- V_lint (soft, optional): syntax/style check; loss proportional to violations.
-
-For PoC: subprocess-based execution with timeouts (no Docker required).
-For production: swap to Docker containers with resource limits behind the same interface.
+Implemented channels:
+- V_tests (hard): run generated code against stdin/stdout test cases
+- V_lint (soft): Python syntax check
+- V_ruff / V_pyright / V_mypy (soft): optional static-analysis oracles
+- V_hypothesis (soft): optional placeholder channel with graceful availability handling
 """
 from __future__ import annotations
 
-from dataclasses import dataclass, field
-from typing import Any, Dict, List, Optional
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Sequence, Tuple
 import json
 import os
 import platform
+import re
 import subprocess
 import sys
 import tempfile
@@ -24,11 +24,23 @@ from agice.contracts import validate_verifier_output
 
 PYTHON_VERSION = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
 
+
 @dataclass
 class VerifierRunnerConfig:
     max_seconds: int = 30
     enable_lint_oracle: bool = False
+    enable_ruff_oracle: bool = False
+    enable_pyright_oracle: bool = False
+    enable_mypy_oracle: bool = False
+    enable_hypothesis_oracle: bool = False
+    static_oracle_timeout_seconds: int = 20
+    strict_external_oracles: bool = False
+    hypothesis_examples: int = 12
     python_bin: str = sys.executable
+    ruff_bin: str = "ruff"
+    pyright_bin: str = "pyright"
+    mypy_bin: str = "mypy"
+
 
 @dataclass
 class TestCase:
@@ -36,6 +48,7 @@ class TestCase:
     expected_output: str
     label: str = ""
 
+
 class VerifierRunner:
     """Runs generated Python code against stdin/stdout test cases."""
 
@@ -53,45 +66,59 @@ class VerifierRunner:
         outs.append(self._run_tests(code, test_cases, task_id=task_id))
         if self.cfg.enable_lint_oracle:
             outs.append(self._run_syntax_check(code))
-        for o in outs:
-            validate_verifier_output(o)
+        if self.cfg.enable_ruff_oracle:
+            outs.append(self._run_ruff(code))
+        if self.cfg.enable_pyright_oracle:
+            outs.append(self._run_pyright(code))
+        if self.cfg.enable_mypy_oracle:
+            outs.append(self._run_mypy(code))
+        if self.cfg.enable_hypothesis_oracle:
+            outs.append(self._run_hypothesis_channel())
+
+        for out in outs:
+            validate_verifier_output(out)
         return outs
 
-    # ------------------------------------------------------------------
-    # V_tests  (hard constraint)
-    # ------------------------------------------------------------------
     def _run_tests(
-        self, code: str, test_cases: List[TestCase], *, task_id: str
+        self,
+        code: str,
+        test_cases: List[TestCase],
+        *,
+        task_id: str,
     ) -> Dict[str, Any]:
         start = time.time()
         results: List[Dict[str, Any]] = []
         total = len(test_cases)
         passed_count = 0
 
-        for idx, tc in enumerate(test_cases):
-            label = tc.label or f"test_{idx}"
-            res = self._execute_code(code, tc.input)
-            actual = res["stdout"].rstrip("\n") + "\n" if res["stdout"] else ""
-            expected = tc.expected_output
-            match = actual.strip() == expected.strip()
-            if match:
+        for idx, test_case in enumerate(test_cases):
+            label = test_case.label or f"test_{idx}"
+            execution = self._execute_code(code, test_case.input)
+            actual = execution["stdout"].rstrip("\n") + "\n" if execution["stdout"] else ""
+            expected = test_case.expected_output
+            passed = actual.strip() == expected.strip()
+            if passed:
                 passed_count += 1
-            results.append({
-                "test_id": label,
-                "passed": match,
-                "input": tc.input[:500] if not match else "",
-                "expected": expected.strip(),
-                "actual": actual.strip(),
-                "stderr": res["stderr"][:2000] if res["stderr"] else "",
-                "exit_code": res["exit_code"],
-                "timed_out": res["timed_out"],
-            })
+            results.append(
+                {
+                    "test_id": label,
+                    "passed": passed,
+                    "input": test_case.input[:500] if not passed else "",
+                    "expected": expected.strip(),
+                    "actual": actual.strip(),
+                    "stderr": execution["stderr"][:2000] if execution["stderr"] else "",
+                    "exit_code": execution["exit_code"],
+                    "timed_out": execution["timed_out"],
+                }
+            )
 
         runtime = time.time() - start
         all_passed = passed_count == total
         crash_count = sum(
-            1 for r in results
-            if not r["passed"] and (r.get("exit_code", 0) != 0 or r.get("timed_out", False))
+            1
+            for result in results
+            if not result["passed"]
+            and (result.get("exit_code", 0) != 0 or result.get("timed_out", False))
         )
         base_failures = total - passed_count
         loss = 0.0 if all_passed else min(1.0, (base_failures + crash_count) / max(total, 1))
@@ -112,72 +139,290 @@ class VerifierRunner:
             "runtime_seconds": runtime,
         }
 
-    # ------------------------------------------------------------------
-    # V_lint  (soft oracle — optional)
-    # ------------------------------------------------------------------
     def _run_syntax_check(self, code: str) -> Dict[str, Any]:
         start = time.time()
         try:
-            compile(code, "<solution>", "exec")
+            compile(code, "<generated>", "exec")
+            errors: List[Dict[str, Any]] = []
             passed = True
             loss = 0.0
-            diag: Dict[str, Any] = {"errors": []}
-        except SyntaxError as e:
+        except SyntaxError as error:
+            errors = [{"line": error.lineno or 0, "msg": str(error.msg)}]
             passed = False
             loss = 1.0
-            diag = {"errors": [{"line": e.lineno, "msg": str(e)}]}
         runtime = time.time() - start
         return {
             "verifier_id": "V_lint",
             "hard_constraint": False,
             "passed": passed,
             "loss": loss,
-            "diagnostics": diag,
-            "logs": {},
+            "diagnostics": {"errors": errors},
+            "logs": {"summary": "syntax check"},
             "version_id": f"python-{PYTHON_VERSION}",
             "runtime_seconds": runtime,
         }
 
-    # ------------------------------------------------------------------
-    # Subprocess execution helper
-    # ------------------------------------------------------------------
-    def _execute_code(self, code: str, stdin_data: str) -> Dict[str, Any]:
-        with tempfile.NamedTemporaryFile(
-            mode="w", suffix=".py", delete=False, encoding="utf-8"
-        ) as f:
-            f.write(code)
-            tmp_path = f.name
+    def _run_ruff(self, code: str) -> Dict[str, Any]:
+        return self._run_static_tool(
+            verifier_id="V_ruff",
+            tool_bin=self.cfg.ruff_bin,
+            args=("check",),
+            code=code,
+            parser=self._parse_ruff_or_mypy_text,
+        )
+
+    def _run_pyright(self, code: str) -> Dict[str, Any]:
+        return self._run_static_tool(
+            verifier_id="V_pyright",
+            tool_bin=self.cfg.pyright_bin,
+            args=("--outputjson",),
+            code=code,
+            parser=self._parse_pyright_json,
+        )
+
+    def _run_mypy(self, code: str) -> Dict[str, Any]:
+        return self._run_static_tool(
+            verifier_id="V_mypy",
+            tool_bin=self.cfg.mypy_bin,
+            args=("--show-error-codes", "--no-error-summary"),
+            code=code,
+            parser=self._parse_ruff_or_mypy_text,
+        )
+
+    def _run_hypothesis_channel(self) -> Dict[str, Any]:
+        start = time.time()
         try:
-            proc = subprocess.run(
-                [self.cfg.python_bin, tmp_path],
-                input=stdin_data,
-                capture_output=True,
-                text=True,
-                timeout=self.cfg.max_seconds,
-                env={**os.environ, "PYTHONHASHSEED": "0"},
+            import hypothesis  # noqa: F401
+            available = True
+        except Exception:
+            available = False
+
+        if not available:
+            return self._external_unavailable(
+                verifier_id="V_hypothesis",
+                tool_name="hypothesis",
+                start_ts=start,
             )
-            return {
-                "stdout": proc.stdout,
-                "stderr": proc.stderr,
-                "exit_code": proc.returncode,
-                "timed_out": False,
-            }
-        except subprocess.TimeoutExpired:
-            return {
-                "stdout": "",
-                "stderr": f"Timeout after {self.cfg.max_seconds}s",
-                "exit_code": 124,
-                "timed_out": True,
-            }
-        except Exception as e:
-            return {
-                "stdout": "",
-                "stderr": str(e),
-                "exit_code": 1,
-                "timed_out": False,
-            }
+
+        runtime = time.time() - start
+        return {
+            "verifier_id": "V_hypothesis",
+            "hard_constraint": False,
+            "passed": True,
+            "loss": 0.0,
+            "diagnostics": {
+                "unavailable": False,
+                "example_count": 0,
+                "error_count": 0,
+                "failures": [],
+                "note": "Hypothesis package available; explicit perturbation harness not configured for this task family.",
+            },
+            "logs": {"summary": "hypothesis channel available"},
+            "version_id": platform.platform(),
+            "runtime_seconds": runtime,
+        }
+
+    def _run_static_tool(
+        self,
+        *,
+        verifier_id: str,
+        tool_bin: str,
+        args: Sequence[str],
+        code: str,
+        parser,
+    ) -> Dict[str, Any]:
+        start = time.time()
+        with tempfile.TemporaryDirectory(prefix="agice_static_") as tmp_dir:
+            source_path = os.path.join(tmp_dir, "candidate.py")
+            with open(source_path, "w", encoding="utf-8") as handle:
+                handle.write(code)
+
+            command = [tool_bin, *args, source_path]
+            try:
+                process = subprocess.run(
+                    command,
+                    stdout=subprocess.PIPE,
+                    stderr=subprocess.PIPE,
+                    text=True,
+                    timeout=self.cfg.static_oracle_timeout_seconds,
+                )
+            except (FileNotFoundError, PermissionError, OSError):
+                return self._external_unavailable(
+                    verifier_id=verifier_id,
+                    tool_name=tool_bin,
+                    start_ts=start,
+                )
+            except subprocess.TimeoutExpired:
+                runtime = time.time() - start
+                return {
+                    "verifier_id": verifier_id,
+                    "hard_constraint": False,
+                    "passed": False,
+                    "loss": 1.0,
+                    "diagnostics": {
+                        "unavailable": False,
+                        "error_count": 1,
+                        "warning_count": 0,
+                        "errors": [{"line": 0, "message": "tool timed out"}],
+                    },
+                    "logs": {"summary": f"{tool_bin} timed out"},
+                    "version_id": platform.platform(),
+                    "runtime_seconds": runtime,
+                }
+
+        runtime = time.time() - start
+        stdout = process.stdout or ""
+        stderr = process.stderr or ""
+        errors, warning_count = parser(stdout=stdout, stderr=stderr)
+        error_count = len(errors)
+        passed = process.returncode == 0 and error_count == 0
+        loss = 0.0 if passed else 1.0
+
+        return {
+            "verifier_id": verifier_id,
+            "hard_constraint": False,
+            "passed": passed,
+            "loss": loss,
+            "diagnostics": {
+                "unavailable": False,
+                "error_count": error_count,
+                "warning_count": warning_count,
+                "errors": errors[:25],
+            },
+            "logs": {
+                "summary": f"rc={process.returncode}",
+                "stdout": stdout[:4000],
+                "stderr": stderr[:4000],
+            },
+            "version_id": platform.platform(),
+            "runtime_seconds": runtime,
+        }
+
+    def _external_unavailable(
+        self,
+        *,
+        verifier_id: str,
+        tool_name: str,
+        start_ts: float,
+    ) -> Dict[str, Any]:
+        runtime = time.time() - start_ts
+        fail_closed = bool(self.cfg.strict_external_oracles)
+        return {
+            "verifier_id": verifier_id,
+            "hard_constraint": False,
+            "passed": not fail_closed,
+            "loss": 1.0 if fail_closed else 0.0,
+            "diagnostics": {
+                "unavailable": True,
+                "error_count": 0 if not fail_closed else 1,
+                "warning_count": 0,
+                "errors": [],
+                "reason": f"{tool_name} not available",
+            },
+            "logs": {"summary": f"{tool_name} unavailable"},
+            "version_id": platform.platform(),
+            "runtime_seconds": runtime,
+        }
+
+    def _execute_code(self, code: str, std_input: str) -> Dict[str, Any]:
+        with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False, encoding="utf-8") as tmp:
+            tmp.write(code)
+            path = tmp.name
+
+        try:
+            try:
+                process = subprocess.run(
+                    [self.cfg.python_bin, path],
+                    input=std_input,
+                    stdout=subprocess.PIPE,
+                    stderr=subprocess.PIPE,
+                    text=True,
+                    timeout=self.cfg.max_seconds,
+                )
+                return {
+                    "stdout": process.stdout,
+                    "stderr": process.stderr,
+                    "exit_code": process.returncode,
+                    "timed_out": False,
+                }
+            except subprocess.TimeoutExpired as error:
+                return {
+                    "stdout": error.stdout or "",
+                    "stderr": (error.stderr or "") + "\n[TIMEOUT]",
+                    "exit_code": -1,
+                    "timed_out": True,
+                }
         finally:
             try:
-                os.unlink(tmp_path)
+                os.remove(path)
             except OSError:
                 pass
+
+    @staticmethod
+    def _parse_ruff_or_mypy_text(*, stdout: str, stderr: str) -> Tuple[List[Dict[str, Any]], int]:
+        text = (stdout + "\n" + stderr).strip()
+        if not text:
+            return [], 0
+        errors: List[Dict[str, Any]] = []
+        warning_count = 0
+        for line in text.splitlines():
+            line = line.strip()
+            if not line:
+                continue
+            lowered = line.lower()
+            if "warning" in lowered:
+                warning_count += 1
+            parsed = VerifierRunner._extract_location(line)
+            errors.append(
+                {
+                    "line": parsed[0],
+                    "column": parsed[1],
+                    "message": line,
+                }
+            )
+        return errors, warning_count
+
+    @staticmethod
+    def _parse_pyright_json(*, stdout: str, stderr: str) -> Tuple[List[Dict[str, Any]], int]:
+        raw = stdout.strip()
+        if not raw:
+            return VerifierRunner._parse_ruff_or_mypy_text(stdout=stdout, stderr=stderr)
+        try:
+            payload = json.loads(raw)
+        except json.JSONDecodeError:
+            return VerifierRunner._parse_ruff_or_mypy_text(stdout=stdout, stderr=stderr)
+
+        diagnostics = payload.get("generalDiagnostics", [])
+        if not isinstance(diagnostics, list):
+            diagnostics = []
+
+        errors: List[Dict[str, Any]] = []
+        warning_count = 0
+        for item in diagnostics:
+            if not isinstance(item, dict):
+                continue
+            severity = str(item.get("severity", "")).lower()
+            if severity == "warning":
+                warning_count += 1
+            start = item.get("range", {}).get("start", {}) if isinstance(item.get("range", {}), dict) else {}
+            line = int(start.get("line", 0)) + 1 if isinstance(start, dict) else 0
+            column = int(start.get("character", 0)) + 1 if isinstance(start, dict) else 0
+            errors.append(
+                {
+                    "line": line,
+                    "column": column,
+                    "message": str(item.get("message", "")),
+                    "severity": severity,
+                }
+            )
+        return errors, warning_count
+
+    @staticmethod
+    def _extract_location(line: str) -> Tuple[int, int]:
+        match = re.search(r":(\\d+):(\\d+):", line)
+        if match:
+            return int(match.group(1)), int(match.group(2))
+        match = re.search(r":(\\d+):", line)
+        if match:
+            return int(match.group(1)), 0
+        return 0, 0
